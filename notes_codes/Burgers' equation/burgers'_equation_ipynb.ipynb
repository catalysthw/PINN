{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "zdHaSVvcBDuW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Physics-Informed Neural Network (PINN) for 1D Burgers' Equation in (x, t)\n",
        "#\n",
        "# PDE (here, with a modified viscosity term):\n",
        "#   u_t + u * u_x - (nu / pi) * u_xx = 0   on   x ∈ [-1, 1], t ∈ [0, 1]\n",
        "#\n",
        "# Initial condition (IC):\n",
        "#   u(x, 0) = -sin(pi * x)\n",
        "#\n",
        "# Boundary conditions (BC, here Dirichlet and homogeneous):\n",
        "#   u(-1, t) = 0,  u(1, t) = 0\n",
        "#\n",
        "# The PINN learns u(x, t) by minimizing:\n",
        "#   - IC loss  (fit u(x,0) to initial_condition(x))\n",
        "#   - BC loss  (fit u at x=-1,1 to boundary_condition)\n",
        "#   - PDE loss (residual of the Burgers equation at collocation points)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1. Define the neural network model (PyTorch)\n",
        "# ============================================================\n",
        "class PINN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple fully-connected neural network (MLP) that takes\n",
        "    (x, t) as input and outputs scalar u(x,t).\n",
        "\n",
        "    Input dimension:  2 (spatial coordinate x, time coordinate t)\n",
        "    Output dimension: 1 (solution u(x,t))\n",
        "\n",
        "    Architecture:\n",
        "      (x,t) -> Linear(2 -> 20) -> Tanh ->\n",
        "               Linear(20->20) -> Tanh ->\n",
        "               Linear(20->20) -> Tanh ->\n",
        "               Linear(20->20) -> Tanh ->\n",
        "               Linear(20->1)  -> u\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(PINN, self).__init__()\n",
        "\n",
        "        # Sequential container for a stack of fully-connected layers + activations.\n",
        "        # This defines the nonlinear mapping (x,t) -> u(x,t).\n",
        "        self.hidden = nn.Sequential(\n",
        "            nn.Linear(2, 20),  # First layer: 2 input features (x, t) -> 20 hidden units\n",
        "            nn.Tanh(),         # Tanh nonlinearity\n",
        "            nn.Linear(20, 20),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(20, 20),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(20, 20),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(20, 1)   # Last layer: 20 -> 1 output (scalar u)\n",
        "        )\n",
        "\n",
        "        # --- Optional: inverse-problem parameter for viscosity ν ---\n",
        "        # If you want to learn ν from data (inverse problem), you can uncomment:\n",
        "        #\n",
        "        #   self.nu = nn.Parameter(torch.rand(1, dtype=torch.float32))\n",
        "        #\n",
        "        # This registers ν as a trainable parameter of the network.\n",
        "        # In the PDE residual, you would then use model.nu instead of a fixed nu.\n",
        "\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        Forward pass of the network.\n",
        "\n",
        "        Inputs:\n",
        "          x : tensor of shape (N, 1), spatial coordinates\n",
        "          t : tensor of shape (N, 1), temporal coordinates\n",
        "\n",
        "        Steps:\n",
        "          1) Concatenate x and t along feature dimension -> shape (N, 2)\n",
        "          2) Pass through the hidden MLP to predict u(x,t).\n",
        "        \"\"\"\n",
        "\n",
        "        # Concatenate x and t into a single (N,2) input tensor:\n",
        "        # dim=1 means we concatenate columns; each row is [x_i, t_i].\n",
        "        inputs = torch.cat([x, t], dim=1)\n",
        "\n",
        "        # Apply the fully-connected network to obtain u predictions\n",
        "        u = self.hidden(inputs)\n",
        "\n",
        "        # Output shape is (N,1)\n",
        "        return u\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. PDE residual: Burgers' equation\n",
        "# ============================================================\n",
        "def pde_residual(x, t, model, nu=0.01):\n",
        "    \"\"\"\n",
        "    Compute the residual of the Burgers' PDE at given collocation points (x,t).\n",
        "\n",
        "    PDE (here written as residual = 0):\n",
        "        u_t + u * u_x - (nu / pi) * u_xx = 0\n",
        "\n",
        "    So the residual is:\n",
        "        r(x,t) = u_t + u * u_x - (nu / pi) * u_xx\n",
        "\n",
        "    Inputs:\n",
        "      x     : tensor of shape (N,1), spatial collocation points\n",
        "      t     : tensor of shape (N,1), temporal collocation points\n",
        "      model : PINN instance that approximates u(x,t)\n",
        "      nu    : viscosity-like coefficient (scalar, float)\n",
        "\n",
        "    Output:\n",
        "      residual : tensor of shape (N,1) representing PDE residual at each point\n",
        "    \"\"\"\n",
        "\n",
        "    # IMPORTANT:\n",
        "    # We must tell PyTorch to track gradients of x and t w.r.t. the computational graph,\n",
        "    # so that autograd can compute derivatives u_t, u_x, u_xx.\n",
        "    x.requires_grad_(True)\n",
        "    t.requires_grad_(True)\n",
        "\n",
        "    # Forward pass: get network prediction u(x,t)\n",
        "    u = model(x, t)  # shape (N,1)\n",
        "\n",
        "    # First derivative w.r.t time: u_t = du/dt\n",
        "    #   autograd.grad arguments:\n",
        "    #     outputs=u\n",
        "    #     inputs=t\n",
        "    #     grad_outputs=torch.ones_like(u)  (vector-Jacobian product, picks elementwise derivative)\n",
        "    #   create_graph=True -> keep graph for higher-order derivatives if needed.\n",
        "    u_t = torch.autograd.grad(\n",
        "        u, t,\n",
        "        grad_outputs=torch.ones_like(u),\n",
        "        create_graph=True)[0]  # shape (N,1)\n",
        "\n",
        "    # First derivative w.r.t space: u_x = du/dx\n",
        "    u_x = torch.autograd.grad(\n",
        "        u, x,\n",
        "        grad_outputs=torch.ones_like(u),\n",
        "        create_graph=True)[0]  # shape (N,1)\n",
        "\n",
        "    # Second derivative w.r.t space: u_xx = d^2u/dx^2\n",
        "    #   We differentiate u_x w.r.t x again.\n",
        "    u_xx = torch.autograd.grad(\n",
        "        u_x, x,\n",
        "        grad_outputs=torch.ones_like(u_x),\n",
        "        create_graph=True\n",
        "    )[0]  # shape (N,1)\n",
        "\n",
        "    # PDE residual:\n",
        "    #   r = u_t + u * u_x - (nu/pi) * u_xx\n",
        "    # The PINN will try to make r(x,t) ≈ 0 at all collocation points.\n",
        "    residual = u_t + u * u_x - nu / (np.pi) * u_xx\n",
        "\n",
        "    return residual  # (N,1)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. Initial and boundary conditions\n",
        "# ============================================================\n",
        "def initial_condition(x):\n",
        "    \"\"\"\n",
        "    Initial condition at t = 0.\n",
        "\n",
        "    IC: u(x, 0) = -sin(pi * x)\n",
        "\n",
        "    Input:\n",
        "      x: tensor of shape (N,1)\n",
        "    Output:\n",
        "      u_IC: tensor of shape (N,1)\n",
        "    \"\"\"\n",
        "    return -torch.sin(np.pi * x)\n",
        "\n",
        "\n",
        "def boundary_condition(x, t):\n",
        "    \"\"\"\n",
        "    Boundary condition u(x,t) on x = -1 and x = 1.\n",
        "\n",
        "    Here: homogeneous Dirichlet BC (u = 0 on the boundary).\n",
        "    We ignore x and just return zero with the same shape as t.\n",
        "\n",
        "    Inputs:\n",
        "      x: boundary x-values (unused)\n",
        "      t: times on the boundary\n",
        "\n",
        "    Output:\n",
        "      tensor of zeros of same shape as t\n",
        "    \"\"\"\n",
        "    return torch.zeros_like(t)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. Create training data: interior collocation grid\n",
        "# ============================================================\n",
        "# Spatial coordinates for initial condition (and also used for PDE collocation)\n",
        "x = torch.linspace(-1, 1, 200).view(-1, 1)  # shape (200,1)\n",
        "# Temporal coordinates for boundary/PDE collocation\n",
        "t = torch.linspace(0, 1, 100).view(-1, 1)   # shape (100,1)\n",
        "\n",
        "# Build a grid of (x,t) collocation points for PDE residual\n",
        "# meshgrid with indexing='xy':\n",
        "#   x_train[i,j] = x_i\n",
        "#   t_train[i,j] = t_j\n",
        "x_train, t_train = torch.meshgrid(\n",
        "    x.squeeze(), t.squeeze(), indexing='xy'\n",
        ")\n",
        "# Flatten the 2D grids into 1D lists of coordinates:\n",
        "#   shape (200*100, 1) = (20000,1)\n",
        "x_train = x_train.reshape(-1, 1)\n",
        "t_train = t_train.reshape(-1, 1)\n",
        "\n",
        "# --- Optional: Inverse problem data (viscosity estimation) ---\n",
        "# If you have reference solution data (x_ref, t_ref, exact), you can create:\n",
        "#\n",
        "# x_data = torch.tensor(x_ref.flatten(), dtype=torch.float32).view(-1, 1)\n",
        "# t_data = torch.tensor(t_ref.flatten(), dtype=torch.float32).view(-1, 1)\n",
        "# u_data = torch.tensor(exact.flatten(), dtype=torch.float32).view(-1, 1)\n",
        "#\n",
        "# And then add a data-fitting loss: MSE between model(x_data,t_data) and u_data.\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. Instantiate the model, optimizer\n",
        "# ============================================================\n",
        "model = PINN()  # by default on CPU; can .to(device) if GPU is used\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6. Training loop\n",
        "# ============================================================\n",
        "num_epochs = 12000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    # ========================================================\n",
        "    # NOTE for inverse problem:\n",
        "    #   For viscosity estimation with data, you would typically\n",
        "    #   use (x_data, t_data) and a data loss term.\n",
        "    #   The PDE residual often uses (x_train, t_train) as here.\n",
        "    #   This comment says that in that setting, some terms may\n",
        "    #   use x_train, t_train instead of x, t.\n",
        "    # ========================================================\n",
        "\n",
        "    # -----------------------\n",
        "    # (a) Initial condition loss\n",
        "    # -----------------------\n",
        "    # Compute model prediction at t=0:\n",
        "    # Collocation points for initial condition: u_pred(x, 0) ≈ u(x, 0)\n",
        "    u_pred = model(x, torch.zeros_like(x))  # input t=0 for all x\n",
        "\n",
        "    # True IC values from the analytical initial condition function\n",
        "    u_true = initial_condition(x)\n",
        "\n",
        "    # Mean squared error of initial condition mismatch\n",
        "    loss_ic = torch.mean((u_pred - u_true) ** 2)\n",
        "\n",
        "    # -----------------------\n",
        "    # (b) Boundary condition loss: Model predictions at boundaries:\n",
        "    # -----------------------\n",
        "    # Collocation points for boundary conditions\n",
        "    # 1) Left boundary x = -1, for all t\n",
        "    x_left = torch.full_like(t, -1.0)  # shape (100,1) all entries = -1\n",
        "    # u(-1,t)\n",
        "    u_pred_left = model(x_left, t)   # shape (100,1)\n",
        "\n",
        "    # 2) Right boundary x =  1, for all t\n",
        "    x_right = torch.full_like(t, 1.0)  # shape (100,1) all entries = 1\n",
        "    # u(1,t)\n",
        "    u_pred_right = model(x_right, t) # shape (100,1)\n",
        "\n",
        "    # Target boundary values (here zero, homogeneous Dirichlet)\n",
        "    u_bc_left = boundary_condition(x_left, t)\n",
        "    u_bc_right = boundary_condition(x_right, t)\n",
        "\n",
        "    # MSE on both boundaries\n",
        "    loss_bc = torch.mean((u_pred_left - u_bc_left) ** 2) + \\\n",
        "              torch.mean((u_pred_right - u_bc_right) ** 2)\n",
        "\n",
        "    # -----------------------\n",
        "    # (c) PDE residual loss\n",
        "    # -----------------------\n",
        "    # Compute PDE residual at interior collocation points (x_train, t_train)\n",
        "    residual = pde_residual(x_train, t_train, model)\n",
        "    # Mean squared residual (PINN forces this toward zero)\n",
        "    loss_pde = torch.mean(residual ** 2)\n",
        "\n",
        "    # -----------------------\n",
        "    # (d) Total loss\n",
        "    # -----------------------\n",
        "    # Here all three components are equally weighted; in practice, you\n",
        "    # might tune weights: loss = w_ic*loss_ic + w_bc*loss_bc + w_pde*loss_pde.\n",
        "    loss = loss_ic + loss_bc + loss_pde\n",
        "\n",
        "    # -----------------------\n",
        "    # (e) Backpropagation and optimizer step\n",
        "    # -----------------------\n",
        "    optimizer.zero_grad()  # clear previous gradients\n",
        "    loss.backward()        # compute gradients d(loss)/d(theta)\n",
        "    optimizer.step()       # update model parameters\n",
        "\n",
        "    # Simple logging every 500 epochs\n",
        "    if (epoch) % 500 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "y8MaPzjoBDuX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7. Evaluation: predict u(x,t) on a test grid for visualization\n",
        "# ============================================================\n",
        "# Create a regular grid in (x,t) for plotting\n",
        "x_test = torch.linspace(-1, 1, 100).view(-1, 1)  # (100,1)\n",
        "t_test = torch.linspace(0, 1, 100).view(-1, 1)   # (100,1)\n",
        "\n",
        "# meshgrid for all combinations of (x_i, t_j)\n",
        "x_test, t_test = torch.meshgrid(\n",
        "    x_test.squeeze(), t_test.squeeze(), indexing='xy'\n",
        ")  # x_test, t_test: shape (100,100)\n",
        "\n",
        "# Flatten to shape (10000,1) so that each row is one (x,t) pair\n",
        "x_test = x_test.reshape(-1, 1)\n",
        "t_test = t_test.reshape(-1, 1)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Predict u(x,t) at these grid points\n",
        "    # Note: .cpu().numpy() is safer if later plotting with matplotlib\n",
        "    u_pred = model(x_test, t_test).cpu().numpy()  # shape (10000,1)\n",
        "\n",
        "# Reshape back to 2D fields for contour plotting:\n",
        "#   x_test_grid[i,j], t_test_grid[i,j], u_pred_grid[i,j]\n",
        "x_test = x_test.numpy().reshape(100, 100)\n",
        "t_test = t_test.numpy().reshape(100, 100)\n",
        "u_pred = u_pred.reshape(100, 100)\n",
        "\n",
        "\n",
        "### Reference dataset\n",
        "## Load the reference solution data\n",
        "data = np.load(\"Burgers.npz\")\n",
        "t_ref, x_ref, exact = data[\"t\"], data[\"x\"], data[\"usol\"].T\n",
        "\n",
        "# Reshape x_ref and t_ref to match the shape of exact\n",
        "x_ref, t_ref = np.meshgrid(x_ref, t_ref)\n",
        "\n",
        "\n",
        "\n",
        "### Plotting: a comparison between reference data and PINN result\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Contour plot: PINN solution\n",
        "contour1 = ax1.contourf(x_test, t_test, u_pred, levels=250, cmap='jet')\n",
        "fig.colorbar(contour1, ax=ax1)\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('t')\n",
        "ax1.set_title('Solution of Burgers\\' Equation using PINNs (CPU)')\n",
        "\n",
        "# Contour plot: reference solution\n",
        "contour2 = ax2.contourf(x_ref, t_ref, exact, levels=250, cmap='jet')\n",
        "fig.colorbar(contour2, ax=ax2)\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('t')\n",
        "ax2.set_title('Reference Solution of Burgers\\' Equation')\n",
        "\n",
        "# Set the same limits for both plots\n",
        "ax1.set_xlim([x_ref.min(), x_ref.max()])\n",
        "ax1.set_ylim([t_ref.min(), t_ref.max()])\n",
        "ax2.set_xlim([x_ref.min(), x_ref.max()])\n",
        "ax2.set_ylim([t_ref.min(), t_ref.max()])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "4sA8_10OBDuY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Plot reference vs PINN solution at selected time slices\n",
        "# ============================================================\n",
        "\n",
        "# Sample time instances at which we want to inspect u(x, t).\n",
        "# Each value in this list corresponds to a horizontal \"slice\" in the (x, t) domain.\n",
        "time_slices = [0.2, 0.4, 0.6, 0.8]  # You can add more time values here if needed.\n",
        "\n",
        "# Total number of time slices we want to visualize.\n",
        "num_plots = len(time_slices)\n",
        "\n",
        "# We fix the number of columns for the subplot layout.\n",
        "# Here, we choose 2 columns so that subplots are arranged in at most 2 per row.\n",
        "cols = 2\n",
        "\n",
        "# Compute how many rows we need in the subplot grid.\n",
        "# Explanation:\n",
        "#   - num_plots // cols : the number of full rows that can be filled completely\n",
        "#   - num_plots % cols  : 1 if there is a partially filled row, 0 otherwise\n",
        "# So if we have an odd number of plots, we add one extra row.\n",
        "rows = (num_plots // cols) + (num_plots % cols)\n",
        "\n",
        "# Create a figure with a grid of subplots having shape (rows, cols).\n",
        "# figsize is scaled with 'rows' so the height grows with the number of rows.\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(12, 5 * rows))\n",
        "\n",
        "# 'axs' is a 2D array of Axes when rows * cols > 1.\n",
        "# Flatten it to a 1D array so we can index subplots as axs[0], axs[1], ...\n",
        "axs = axs.flatten()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Loop over the desired time slices and plot each in a subplot\n",
        "# ------------------------------------------------------------\n",
        "for i, t_val in enumerate(time_slices):\n",
        "    # --------------------------------------------------------\n",
        "    # 1) Extract the reference solution at time t ~ t_val\n",
        "    # --------------------------------------------------------\n",
        "\n",
        "    # 't_ref' is assumed to be an array of reference time values with shape (Nt, 1).\n",
        "    # We find the index of the time in t_ref that is closest to the desired t_val.\n",
        "    # This allows us to approximate the reference solution at t = t_val\n",
        "    # even if t_val is not exactly one of the t_ref grid points.\n",
        "    idx_ref = np.argmin(np.abs(t_ref[:, 0] - t_val))\n",
        "\n",
        "    # 'exact' is assumed to have shape (Nt, Nx), where:\n",
        "    #   Nt : number of time steps in the reference solution\n",
        "    #   Nx : number of spatial grid points.\n",
        "    # Selecting 'exact[idx_ref, :]' gives the spatial profile u(x, t_ref[idx_ref])\n",
        "    # at the chosen time slice closest to t_val.\n",
        "    u_ref_slice = exact[idx_ref, :]\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 2) Compute the PINN prediction at the same time t_val\n",
        "    # --------------------------------------------------------\n",
        "\n",
        "    # We create a spatial grid x_slice over [-1, 1], with 100 points.\n",
        "    # Shape: (100, 1)\n",
        "    x_slice = np.linspace(-1, 1, 100).reshape(-1, 1)\n",
        "\n",
        "    # For each x in x_slice, we want to evaluate u(x, t_val).\n",
        "    # So we build t_slice by repeating t_val for all 100 x-points.\n",
        "    # Shape: (100, 1)\n",
        "    t_slice = t_val * np.ones((100, 1))\n",
        "\n",
        "    # Switch the model into evaluation mode.\n",
        "    # This is important if the model contains layers like Dropout or BatchNorm.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation, since we are only doing inference (no backprop).\n",
        "    with torch.no_grad():\n",
        "        # Convert x_slice and t_slice from NumPy arrays to PyTorch tensors\n",
        "        # with dtype float32. Shape remains (100, 1) for both.\n",
        "        x_tensor = torch.tensor(x_slice, dtype=torch.float32)\n",
        "        t_tensor = torch.tensor(t_slice, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass through the PINN model:\n",
        "        #   model(x_tensor, t_tensor) -> predicted u(x, t_val)\n",
        "        # The output 'u_pinn_slice' is a tensor of shape (100, 1).\n",
        "        u_pinn_slice = model(x_tensor, t_tensor).numpy()  # convert to NumPy for plotting\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 3) Plot reference vs PINN solution in subplot axs[i]\n",
        "    # --------------------------------------------------------\n",
        "\n",
        "    # x_ref is assumed to contain the spatial grid used by the reference solution.\n",
        "    # For example, x_ref might have shape (1, Nx) or (Nt, Nx). Here we read the first row.\n",
        "    # u_ref_slice is the reference solution at t ~ t_val, with shape (Nx,).\n",
        "    axs[i].plot(x_ref[0, :], u_ref_slice, 'r-', label='Reference Solution')\n",
        "\n",
        "    # Plot the PINN prediction at the same (or very similar) spatial points.\n",
        "    # x_slice is shape (100, 1), u_pinn_slice is shape (100, 1).\n",
        "    axs[i].plot(x_slice, u_pinn_slice, 'b--', label='PINN Solution')\n",
        "\n",
        "    # Label the axes for the current subplot.\n",
        "    axs[i].set_xlabel('x')   # horizontal axis: spatial coordinate\n",
        "    axs[i].set_ylabel('u')   # vertical axis: solution value u(x, t)\n",
        "\n",
        "    # Title indicates which time slice this subplot corresponds to.\n",
        "    axs[i].set_title(f'Solution at t = {t_val}')\n",
        "\n",
        "    # Add a legend to distinguish reference vs PINN curves.\n",
        "    axs[i].legend()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Remove any unused subplot axes (when rows*cols > num_plots)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# If rows*cols > num_plots, we have extra axes that are not used.\n",
        "# For example, with 3 plots and a 2x2 grid, axs has length 4 but we only used indices 0,1,2.\n",
        "# The following loop removes the unused axes from the figure to avoid empty subplots.\n",
        "for j in range(num_plots, len(axs)):\n",
        "    fig.delaxes(axs[j])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Final layout tweaks and display\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Automatically adjust spacing between subplots so that labels, titles, and legends\n",
        "# do not overlap.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Render the complete figure with all subp\n",
        "::contentReference[oaicite:0]{index=0}\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "eLtfMGkkBDuZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "dff-BSeXBDuZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Visualization of collocation points: interior, IC, BC collocation points\n",
        "# ============================================================\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Interior collocation points (for PDE residual)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Create 1D grids for space (x) and time (t).\n",
        "# Here we choose 25 points in x ∈ [-1, 1] and 25 points in t ∈ [0, 1].\n",
        "# -> resolution in each dimension is a design choice (DL engineer can tune this).\n",
        "x_values = torch.linspace(-1, 1, 25).view(-1, 1)  # shape: (25, 1), spatial grid\n",
        "t_values = torch.linspace(0, 1, 25).view(-1, 1)   # shape: (25, 1), temporal grid\n",
        "\n",
        "# Create a 2D meshgrid of (x, t) pairs.\n",
        "# x_values.squeeze(): shape (25,) / t_values.squeeze(): shape (25,)\n",
        "# indexing='xy' means:\n",
        "#   - first output (x_collocation) corresponds to x-axis (horizontal)\n",
        "#   - second output (t_collocation) corresponds to y-axis (vertical)\n",
        "# The resulting x_collocation, t_collocation have shape (25, 25).\n",
        "x_collocation, t_collocation = torch.meshgrid(\n",
        "    x_values.squeeze(),\n",
        "    t_values.squeeze(),\n",
        "    indexing='xy'\n",
        ")\n",
        "\n",
        "# Reshape the meshgrid into a list of collocation points.\n",
        "# (25, 25) -> (25*25, 1) = (625, 1)\n",
        "# Each row of (x_collocation, t_collocation) now corresponds to one (x_i, t_i) point\n",
        "# inside the spatio-temporal domain Ω = [-1,1] × [0,1].\n",
        "x_collocation = x_collocation.reshape(-1, 1)\n",
        "t_collocation = t_collocation.reshape(-1, 1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Boundary points (for Dirichlet/Neumann boundary conditions)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# For Burgers / heat equation in 1D, typical boundaries are at x = -1 and x = 1.\n",
        "# We will generate boundary points for all time values t ∈ [0,1]\n",
        "# on both the left and right boundary.\n",
        "\n",
        "# Left boundary: x = -1 for all t_values\n",
        "# torch.full_like(t_values, -1) creates a tensor with the same shape as t_values\n",
        "# filled with -1 → x = -1 at each time point.\n",
        "x_boundary_left = torch.full_like(t_values, -1)   # shape: (25,1)\n",
        "\n",
        "# Right boundary: x = 1 for all t_values\n",
        "x_boundary_right = torch.full_like(t_values, 1)   # shape: (25,1)\n",
        "\n",
        "# Time coordinates for boundary points are exactly t_values.\n",
        "# So boundary points are:\n",
        "#   Left:  (x=-1, t=t_values[k])\n",
        "#   Right: (x= 1, t=t_values[k])\n",
        "t_boundary_points = t_values                      # shape: (25,1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Initial condition points (for u(x, t=0))\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Initial condition is usually given at t = 0 for all x in [-1,1].\n",
        "# So we set t = 0 for all spatial points.\n",
        "t_initial_condition = torch.zeros_like(x_values)  # shape: (25,1), all zeros -> t = 0\n",
        "\n",
        "# For the IC, x just spans over the spatial grid.\n",
        "x_initial_condition = x_values                    # shape: (25,1), x ∈ [-1,1]\n",
        "\n",
        "# At these points (x_initial_condition, t_initial_condition),\n",
        "# we enforce u(x, 0) = u0(x).\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Convert to NumPy for plotting (Matplotlib expects NumPy)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Interior collocation points (PDE residual)\n",
        "x_collocation_np = x_collocation.numpy()\n",
        "t_collocation_np = t_collocation.numpy()\n",
        "\n",
        "# Initial condition points\n",
        "x_initial_condition_np = x_initial_condition.numpy()\n",
        "t_initial_condition_np = t_initial_condition.numpy()\n",
        "\n",
        "# Boundary points\n",
        "x_boundary_left_np = x_boundary_left.numpy()\n",
        "x_boundary_right_np = x_boundary_right.numpy()\n",
        "t_boundary_points_np = t_boundary_points.numpy()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Visualize the geometry and different sets of points\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Interior (domain) points: where PDE residual is enforced\n",
        "plt.scatter(\n",
        "    x_collocation_np, t_collocation_np,\n",
        "    label='Domain Points (PDE residual)',\n",
        "    color='blue', s=10\n",
        ")\n",
        "\n",
        "# Initial condition points: t = 0 line\n",
        "plt.scatter(\n",
        "    x_initial_condition_np, t_initial_condition_np,\n",
        "    label='Initial Condition Points (t = 0)',\n",
        "    color='green', s=30\n",
        ")\n",
        "\n",
        "# Left boundary: x = -1 over all t\n",
        "plt.scatter(\n",
        "    x_boundary_left_np, t_boundary_points_np,\n",
        "    label='Left Boundary Points (x = -1)',\n",
        "    color='red', s=30\n",
        ")\n",
        "\n",
        "# Right boundary: x = 1 over all t\n",
        "plt.scatter(\n",
        "    x_boundary_right_np, t_boundary_points_np,\n",
        "    label='Right Boundary Points (x = 1)',\n",
        "    color='orange', s=30\n",
        ")\n",
        "\n",
        "# Label axes: x-axis is space, y-axis is time\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('t')\n",
        "plt.title('Collocation, Initial, and Boundary Points in (x, t) Domain')\n",
        "\n",
        "# Put the legend outside the plot to avoid cluttering the figure area\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "_QdUAIBKBDuZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "fjfy4Va7BDua"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "### DeepXDE utilized\n",
        "\n",
        "!pip install deepxde\n",
        "\n",
        "import deepxde as dde\n",
        "dde.backend.set_default_backend(\"pytorch\")\n",
        "from deepxde.backend import torch, backend_name\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "'''\n",
        "\"# DeepXDE will internally create tf.keras layers with these specs.\"\n",
        "If there is any issue with torch.sin or other torch.###,\n",
        "just replace it with tf.sin or tf.###.\n",
        "'''\n",
        "\n",
        "# # For reproducibility: fix all random seeds used by DeepXDE / TensorFlow backend\n",
        "# dde.config.set_random_seed(1234)\n"
      ],
      "metadata": {
        "id": "6kvzNAIZBFcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deepxde as dde\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "# PDE: 1D viscous Burgers' equation u_t + u u_x - ν u_xx = 0\n",
        "def pde(x, y):\n",
        "    # First derivatives: y_x, y_t\n",
        "    dy_x = dde.grad.jacobian(y, x, i=0, j=0)\n",
        "    dy_t = dde.grad.jacobian(y, x, i=0, j=1)\n",
        "    # Second derivative in space: y_xx\n",
        "    dy_xx = dde.grad.hessian(y, x, i=0, j=0)\n",
        "    return dy_t + y * dy_x - 0.01 / np.pi * dy_xx\n",
        "\n",
        "\n",
        "# ---- Boundary condition helpers ----\n",
        "def bc_fn(x, on_boundary):\n",
        "    \"\"\"Return True on spatial boundary x = -1 or x = 1.\"\"\"\n",
        "    if not on_boundary:\n",
        "        return False\n",
        "    return np.isclose(x[0], -1) or np.isclose(x[0], 1)\n",
        "\n",
        "def bc_val(x):\n",
        "    \"\"\"Dirichlet boundary value u(t, ±1) = 0.\"\"\"\n",
        "    return 0\n",
        "\n",
        "# ---- Initial condition helpers ----\n",
        "def ic_fn(x, on_initial):\n",
        "    \"\"\"Return True on initial line t = 0 for all x.\"\"\"\n",
        "    return on_initial and dde.utils.isclose(x[1], 0)\n",
        "\n",
        "def ic_val(x):\n",
        "    \"\"\"Initial profile u(0, x) = -sin(π x).\"\"\"\n",
        "    return -np.sin(np.pi * x[:, 0:1])\n",
        "\n",
        "# ---- Geometry in (x, t) ----\n",
        "geom = dde.geometry.Interval(-1, 1)\n",
        "timedomain = dde.geometry.TimeDomain(0, 1)\n",
        "geomtime = dde.geometry.GeometryXTime(geom, timedomain)\n",
        "\n",
        "# ---- BC and IC objects ----\n",
        "bc = dde.icbc.DirichletBC(geomtime, bc_val, bc_fn)\n",
        "ic = dde.icbc.IC(geomtime, ic_val, ic_fn)\n",
        "\n",
        "# ---- Data and model definition ----\n",
        "data = dde.data.TimePDE(\n",
        "    geomtime,\n",
        "    pde,\n",
        "    [bc, ic],\n",
        "    num_domain=2540,   # interior collocation points\n",
        "    num_boundary=80,   # boundary points\n",
        "    num_initial=160,   # initial-condition points\n",
        ")\n",
        "\n",
        "layer_size = [2] + [20] * 3 + [1]   # (x,t) → 3 hidden layers → u\n",
        "activation = \"tanh\"\n",
        "initializer = \"Glorot uniform\"\n",
        "net = dde.nn.FNN(layer_size, activation, initializer)\n",
        "model = dde.Model(data, net)\n",
        "\n",
        "\n",
        "# Custom metric: always evaluate L2 error on external reference grid\n",
        "def l2_rel_err_ext(_, y_pred):\n",
        "    \"\"\"L2 relative error on (X_ref, Y_ref) instead of internal DeepXDE test set.\"\"\"\n",
        "    y_test_pred = model.predict(X_ref)\n",
        "    return dde.metrics.l2_relative_error(Y_ref, y_test_pred)\n",
        "\n",
        "\n",
        "# ---- Compile and train with Adam ----\n",
        "model.compile(\n",
        "    \"adam\",\n",
        "    lr=1e-3,\n",
        "    metrics=[l2_rel_err_ext])\n",
        "\n",
        "losshistory, train_state = model.train(iterations=15000)\n",
        "\n",
        "\n",
        "# ---- Final evaluation on external test data ----\n",
        "y_pred = model.predict(X_ref)\n",
        "final_l2_error = dde.metrics.l2_relative_error(Y_ref, y_pred)\n",
        "f = model.predict(X_ref, operator=pde)  # PDE residual at test points\n",
        "\n",
        "print(\"Final L2 relative error (on external test data): {:.2e}\".format(final_l2_error))\n",
        "\n",
        "# ---- Generate grid and plot results ----\n",
        "t = np.linspace(0, 1, 100)\n",
        "x = np.linspace(-1, 1, 256)\n",
        "xx, tt = np.meshgrid(x, t)\n",
        "X = np.vstack((xx.ravel(), tt.ravel())).T\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "y_pred_reshaped = y_pred.reshape(len(t), len(x))\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Reference solution\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.contourf(xx, tt, exact, levels=50, cmap=\"jet\")\n",
        "plt.colorbar(label=\"u\")\n",
        "plt.title(\"Reference Solution of Burgers' Equation\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"t\")\n",
        "\n",
        "# PINN solution\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.contourf(xx, tt, y_pred_reshaped, levels=50, cmap=\"jet\")\n",
        "plt.colorbar(label=\"u\")\n",
        "plt.title(\"Solution of Burgers' Equation using PINNs (CPU)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"t\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XrAPG817BFZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time instances for line plots\n",
        "time_slices = [0.0, 0.25, 0.50, 0.75, 1.0]\n",
        "\n",
        "# Determine rows and columns for subplots\n",
        "num_plots = len(time_slices)\n",
        "cols = 2  # Two columns for better arrangement\n",
        "rows = (num_plots + cols - 1) // cols  # Ceiling division for rows\n",
        "\n",
        "# Create the subplot grid\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(12, 5 * rows))  # Adjust height based on rows\n",
        "axs = axs.flatten()  # Flatten for easy indexing\n",
        "\n",
        "# Loop through time slices and plot solutions\n",
        "for i, t_val in enumerate(time_slices):\n",
        "    # Find the index corresponding to the closest time value\n",
        "    idx = np.argmin(np.abs(t - t_val))\n",
        "\n",
        "    # Extract solutions at the specific time slice\n",
        "    y_analytical = exact[idx, :]\n",
        "    y_pinn = y_pred_reshaped[idx, :]\n",
        "\n",
        "    # Plot analytical and predicted solutions\n",
        "    axs[i].plot(x, y_analytical, 'r-', label='Analytical Solution')\n",
        "    axs[i].plot(x, y_pinn, 'b--', label='PINN Solution')\n",
        "    axs[i].set_xlabel('x')\n",
        "    axs[i].set_ylabel('u')\n",
        "    axs[i].set_title(f'Solution at t = {t_val}')\n",
        "    axs[i].legend()\n",
        "\n",
        "# Remove any empty subplots if the grid has extra spaces\n",
        "for j in range(num_plots, len(axs)):\n",
        "    fig.delaxes(axs[j])\n",
        "\n",
        "# Adjust layout for neatness\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0PIOPa1rBFWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOPohC3UBFTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BdauIB7mBFRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}