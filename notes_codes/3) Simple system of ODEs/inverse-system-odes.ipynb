{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Device selection: use GPU if available, otherwise fall back to CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# -------------------------------------------------------------\n# PINN model with learnable ODE coefficients c1, c2\n#   System:\n#     dx/dt + c1 * x + y = 0\n#     dy/dt + c2 * x + 2y = 0\n#   The network outputs [x(t), y(t)], and we learn both:\n#     - the function x(t), y(t)\n#     - the coefficients c1, c2\n# -------------------------------------------------------------\nclass PINN(nn.Module):\n    def __init__(self):\n        super(PINN, self).__init__()\n\n        # A small fully-connected network (MLP) that takes time t as input (1D)\n        # and outputs two components [x(t), y(t)].\n        self.net = nn.Sequential(\n            nn.Linear(1, 64),  # Input: t (1D) → 64 hidden units\n            nn.Tanh(),         # Nonlinear activation (smooth, good for PINNs)\n            nn.Linear(64, 64),\n            nn.Tanh(),\n            nn.Linear(64, 2)   # Output: 2 values → x(t), y(t)\n        )\n\n        # -----------------------------\n        # Learnable coefficients c1, c2 for the ODE\n        # -----------------------------\n        # We initialize them randomly in (0,10) using torch.rand.\n        # nn.Parameter registers them as trainable parameters of the model,\n        # so optimizer will update them during training.\n        self.coeff_c1 = nn.Parameter(\n            torch.tensor(\n                10 * torch.rand(1, dtype=torch.float32),  # random float in [0,10)\n                requires_grad=True,\n                device=device))\n\n        self.coeff_c2 = nn.Parameter(\n            torch.tensor(\n                10 * torch.rand(1, dtype=torch.float32),  # random float in [0,10)\n                requires_grad=True,\n                device=device))\n\n    def forward(self, t):\n        \"\"\"\n        Forward pass: map time t → [x(t), y(t)].\n\n        Input:\n          t : tensor of shape (N, 1) on the chosen device\n        Output:\n          tensor of shape (N, 2) where:\n            output[:, 0] ≈ x(t)\n            output[:, 1] ≈ y(t)\n        \"\"\"\n        return self.net(t)\n\n\n# -------------------------------------------------------------\n# Loss function: PDE residual + initial condition + data\n# -------------------------------------------------------------\ndef loss_fn(model, t, t_data, x_data, y_data):\n    \"\"\"\n    Compute total loss for the PINN:\n\n      1) PDE residual loss:\n           dx/dt + c1*x + y ≈ 0\n           dy/dt + c2*x + 2y ≈ 0\n\n      2) Initial condition loss at t = 0:\n           x(0) = 1,  y(0) = 0\n\n      3) Data loss:\n         fit (x(t), y(t)) to given analytical/data samples (x_data, y_data)\n         at times t_data.\n\n    Inputs:\n      model   : PINN instance\n      t       : collocation points in time for enforcing PDE (shape: (N,1))\n      t_data  : time points where we have reference data\n      x_data  : reference x(t_data)\n      y_data  : reference y(t_data)\n\n    Output:\n      scalar loss tensor\n    \"\"\"\n\n    # Enable gradients w.r.t t so that autograd can compute dx/dt, dy/dt\n    t.requires_grad = True\n\n    # ---------------------------------------------------------\n    # 1) Forward pass at all collocation points t\n    # ---------------------------------------------------------\n    pred = model(t)          # shape (N,2)\n    x = pred[:, 0:1]         # shape (N,1), x(t)\n    y = pred[:, 1:2]         # shape (N,1), y(t)\n\n    # ---------------------------------------------------------\n    # 2) Compute time derivatives via autograd\n    # ---------------------------------------------------------\n    # dx/dt = d x(t) / dt\n    dx_dt = torch.autograd.grad(\n        outputs=x,\n        inputs=t,\n        grad_outputs=torch.ones_like(x),\n        create_graph=True\n    )[0]\n\n    # dy/dt = d y(t) / dt\n    dy_dt = torch.autograd.grad(\n        outputs=y,\n        inputs=t,\n        grad_outputs=torch.ones_like(y),\n        create_graph=True\n    )[0]\n\n    # ---------------------------------------------------------\n    # 3) PDE residuals with learnable coefficients c1, c2\n    #    We want these residuals to be ≈ 0.\n    # ---------------------------------------------------------\n    # For dx/dt + c1*x + y = 0 → residual_x = dx/dt + c1*x + y\n    res_x = dx_dt + model.coeff_c1 * x + y\n\n    # For dy/dt + c2*x + 2y = 0 → residual_y = dy/dt + c2*x + 2*y\n    res_y = dy_dt + model.coeff_c2 * x + 2 * y\n\n    # Mean squared residual over all collocation points\n    pde_loss = torch.mean(res_x ** 2) + torch.mean(res_y ** 2)\n\n    # ---------------------------------------------------------\n    # 4) Initial condition loss at t = 0\n    #    We assume t is sorted and t[0] corresponds to t=0.\n    #    Conditions: x(0) = 1, y(0) = 0.\n    # ---------------------------------------------------------\n    init_loss_x = (x[0] - 1) ** 2   # penalty for x(0) ≠ 1\n    init_loss_y = (y[0] - 0) ** 2   # penalty for y(0) ≠ 0\n\n    # ---------------------------------------------------------\n    # 5) Data loss term: fit to known analytical solutions\n    # ---------------------------------------------------------\n    # Forward pass at data time points t_data\n    pred_data = model(t_data)\n    x_pred_data = pred_data[:, 0:1]\n    y_pred_data = pred_data[:, 1:2]\n\n    # MSE between predicted and true data\n    data_loss = torch.mean((x_pred_data - x_data) ** 2) + \\\n                torch.mean((y_pred_data - y_data) ** 2)\n\n    # ---------------------------------------------------------\n    # 6) Total loss: weighted combination\n    #    Here PDE residual is given extra weight (×2),\n    #    but weighting is a hyperparameter you can tune.\n    # ---------------------------------------------------------\n    loss = 2 * pde_loss + (init_loss_x + init_loss_y) + data_loss\n\n    return loss\n\n\n# -------------------------------------------------------------\n# Training function (custom loop)\n# -------------------------------------------------------------\ndef train(model, optimizer, t, t_data, x_data, y_data, epochs):\n    \"\"\"\n    Custom training loop for the PINN.\n\n    Inputs:\n      model    : PINN instance\n      optimizer: torch optimizer (e.g., Adam)\n      t        : collocation times for PDE enforcement (N,1)\n      t_data   : time points with data\n      x_data   : reference x(t_data)\n      y_data   : reference y(t_data)\n      epochs   : number of gradient descent steps\n    \"\"\"\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n\n        # Compute current loss (PDE + IC + data)\n        loss = loss_fn(model, t, t_data, x_data, y_data)\n\n        # Backpropagation\n        loss.backward()\n\n        # Parameter update (weights, biases, and coeff_c1, coeff_c2)\n        optimizer.step()\n\n        # Occasionally print progress and current learned c1, c2\n        if epoch % 500 == 0:\n            print(\n                f'Epoch {epoch}, '\n                f'Loss: {loss.item():.6e}, '\n                f'coeff_c1: {model.coeff_c1.item():.6f}, '\n                f'coeff_c2: {model.coeff_c2.item():.6f}'\n            )\n\n\n# -------------------------------------------------------------\n# Create the model and optimizer\n# -------------------------------------------------------------\nmodel = PINN().to(device)                         # move model to CPU or GPU\noptimizer = torch.optim.Adam(model.parameters(),  # optimize all model params\n                             lr=0.001)\n\n\n# -------------------------------------------------------------\n# Generate synthetic data from analytical solutions\n#   These play role of \"ground truth\" to fit and to infer c1, c2.\n# -------------------------------------------------------------\n# Time points where we sample the analytical solution\nt_data = np.linspace(0, 5, 100)[:, None]  # shape: (100,1)\n\n# Analytical solutions for x(t) and y(t) for the system:\n#   dx/dt + 2x + y = 0\n#   dy/dt + x + 2y = 0\n# with IC: x(0)=1, y(0)=0\nx_data = 0.5 * np.exp(-t_data) + 0.5 * np.exp(-3 * t_data)\ny_data = -0.5 * np.exp(-t_data) + 0.5 * np.exp(-3 * t_data)\n\n# Convert numpy arrays to torch tensors on the chosen device\nt_data_tensor = torch.tensor(t_data, dtype=torch.float32, device=device)\nx_data_tensor = torch.tensor(x_data, dtype=torch.float32, device=device)\ny_data_tensor = torch.tensor(y_data, dtype=torch.float32, device=device)\n\n\n# -------------------------------------------------------------\n# Training domain (collocation points for PDE residual)\n# -------------------------------------------------------------\n# Use 100 points in [0,5] as collocation times for enforcing the ODE.\nt_train = torch.linspace(0, 5, 100).view(-1, 1).to(device)\n\n\n# -------------------------------------------------------------\n# Train the model (learn x(t), y(t), c1, c2)\n# -------------------------------------------------------------\ntrain(\n    model,\n    optimizer,\n    t_train,\n    t_data_tensor,\n    x_data_tensor,\n    y_data_tensor,\n    epochs=15000\n)\n\n\n# -------------------------------------------------------------\n# Prediction on a finer time grid for plotting / evaluation\n# -------------------------------------------------------------\nt_test = torch.linspace(0, 5, 500).view(-1, 1).to(device)\n\nwith torch.no_grad():  # no gradient tracking needed for inference\n    pred = model(t_test)           # shape (500,2)\n    x_pred = pred[:, 0].cpu().numpy()  # predicted x(t) as numpy array\n    y_pred = pred[:, 1].cpu().numpy()  # predicted y(t) as numpy array\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analytical solutions for comparison\nx_true = 0.5 * np.exp(-t_test.cpu().numpy()) + 0.5 * np.exp(-3 * t_test.cpu().numpy())\ny_true = -0.5 * np.exp(-t_test.cpu().numpy()) + 0.5 * np.exp(-3 * t_test.cpu().numpy())\n\n\n# Plotting results\nplt.figure(figsize=(12, 5))\n\n# Plot x(t)\nplt.subplot(1, 2, 1)\nplt.plot(t_test.cpu(), x_true, label='Analytical x(t)', color='red')\nplt.plot(t_test.cpu(), x_pred, '--', label='PINNs x(t)', color='blue')\nplt.title(r'PINNs vs Analytical Solution $x(t)$')\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$x(t)$')\nplt.grid(True)\nplt.legend()\n\n# Plot y(t)\nplt.subplot(1, 2, 2)\nplt.plot(t_test.cpu(), y_true, label='Analytical y(t)', color='red')\nplt.plot(t_test.cpu(), y_pred, '--', label='PINNs y(t)', color='blue')\nplt.title(r'PINNs vs Analytical Solution $y(t)$')\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$y(t)$')\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------------------\n# Compare learned coefficients (c1, c2) with their reference values\n# -------------------------------------------------------------\n\n# Extract the learned values of c1 and c2 from the trained model.\n# .item() converts the scalar tensor into a Python float.\nlearned_coeff_c1 = model.coeff_c1.item()\nlearned_coeff_c2 = model.coeff_c2.item()\n\n# Ground-truth (reference) coefficients from the original ODE system:\n#   dx/dt + 2x + y = 0   →  c1 = 2\n#   dy/dt + 1*x + 2y = 0 →  c2 = 1\nreference_c1 = 2\nreference_c2 = 1\n\n# -------------------------------------------------------------\n# Compute percentage error and \"accuracy\" for c1\n# -------------------------------------------------------------\n# Percentage error for c1:\n#   error_c1 = |c1_learned - c1_ref| / |c1_ref| × 100 (%)\nerror_c1 = abs((learned_coeff_c1 - reference_c1) / reference_c1) * 100\n\n# Define percentage accuracy for c1 as 100% - error.\n# (This is a simple measure of how close the learned c1 is to the true c1.)\naccuracy_c1 = 100 - error_c1\n\n# -------------------------------------------------------------\n# Compute percentage error and \"accuracy\" for c2\n# -------------------------------------------------------------\n# Percentage error for c2:\n#   error_c2 = |c2_learned - c2_ref| / |c2_ref| × 100 (%)\nerror_c2 = abs((learned_coeff_c2 - reference_c2) / reference_c2) * 100\n\n# Percentage accuracy for c2, defined analogously to c1.\naccuracy_c2 = 100 - error_c2\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}