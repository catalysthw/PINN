{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T03:51:19.878530Z","iopub.execute_input":"2025-12-02T03:51:19.878861Z","iopub.status.idle":"2025-12-02T03:51:43.558502Z","shell.execute_reply.started":"2025-12-02T03:51:19.878827Z","shell.execute_reply":"2025-12-02T03:51:43.557359Z"}},"outputs":[{"name":"stderr","text":"2025-12-02 03:51:21.988850: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764647482.293306      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764647482.374581      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# -----------------------------\n# PINN model definition (TensorFlow / Keras)\n# -----------------------------\nclass PINN(tf.keras.Model):\n    \"\"\"\n    Simple fully-connected network that takes time t as input\n    and outputs two variables: x(t) and y(t).\n    We subclass tf.keras.Model so that:\n      - layers are registered as trainable variables\n      - we can use model(...) like a normal Keras model\n      - it integrates nicely with Keras optimizers / saving / etc.\n    \"\"\"\n    def __init__(self):\n        super(PINN, self).__init__()\n        # First hidden layer: 64 units, tanh activation\n        self.hidden_1 = tf.keras.layers.Dense(64, activation='tanh')\n        # Second hidden layer: 64 units, tanh activation\n        self.hidden_2 = tf.keras.layers.Dense(64, activation='tanh')\n        # Output layer: 2 units, linear activation -> [x(t), y(t)]\n        self.output_3 = tf.keras.layers.Dense(2, activation=None)\n\n    def call(self, t):\n        \"\"\"\n        Forward pass of the network.\n        Input:\n          t : shape (N, 1), time points\n        Output:\n          shape (N, 2), where:\n            output[:, 0] ≈ x(t)\n            output[:, 1] ≈ y(t)\n        \"\"\"\n        z = self.hidden_1(t)\n        z = self.hidden_2(z)\n        return self.output_3(z)\n\n\n# -----------------------------\n# Loss function (PDE/ODE residual + initial conditions)\n# -----------------------------\ndef loss_fn(model, t):\n    \"\"\"\n    Compute PINN loss for the system:\n        dx/dt + 2x + y = 0\n        dy/dt + x + 2y = 0\n    with initial conditions:\n        x(0) = 1,  y(0) = 0\n\n    Inputs:\n      model : PINN instance\n      t     : shape (N, 1) tensor of time points in [0, T]\n    Output:\n      scalar loss (tf.Tensor)\n    \"\"\"\n\n    # We need derivatives of x(t) and y(t) w.r.t. t.\n    # Two GradientTapes are used, each watching t, so that we can\n    # compute dx/dt and dy/dt separately.\n    with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n        # Tell both tapes that t is a \"variable\" they should differentiate w.r.t.\n        tape1.watch(t)\n        tape2.watch(t)\n\n        # Forward pass: network outputs [x(t), y(t)] for all collocation points\n        output = model(t)           # shape (N, 2)\n        '''if output[:,0] is used, its size will be (N,) tensor instead of (N,1)\n        '''\n        x = output[:, 0:1]          # shape (N, 1) -> keep 2D tensor\n        y = output[:, 1:2]          # shape (N, 1)\n\n        # First-order derivatives w.r.t. t\n        # dx_dt[i] ~ d x(t_i) / d t_i\n        dx_dt = tape1.gradient(x, t)\n        dy_dt = tape2.gradient(y, t)\n\n        # ODE residuals (what should be zero for an exact solution)\n        # From:\n        #   dx/dt + 2x +  y = 0  -> residual_x = dx/dt + 2x + y\n        #   dy/dt +  x + 2y = 0  -> residual_y = dy/dt + x + 2y\n        res_x = dx_dt + 2.0 * x + y\n        res_y = dy_dt + x + 2.0 * y\n\n        # Initial-condition residuals at t = 0.\n        # We assume that t[0] = 0, so x[0] and y[0] correspond to x(0), y(0).\n        # x(0) should be 1,  y(0) should be 0.\n        init_loss_x = tf.square(x[0] - 1.0)\n        init_loss_y = tf.square(y[0] - 0.0)\n\n        # PDE/ODE residual loss: mean-squared error over all collocation points\n        loss_res_x = tf.reduce_mean(tf.square(res_x))\n        loss_res_y = tf.reduce_mean(tf.square(res_y))\n\n        # Total loss = ODE residuals + initial-condition penalties\n        loss = loss_res_x + loss_res_y + init_loss_x + init_loss_y\n\n    return loss\n\n\n# -----------------------------\n# Training loop\n# -----------------------------\ndef train(model, t, epochs, optimizer):\n    \"\"\"\n    Simple custom training loop.\n\n    Inputs:\n      model     : PINN instance\n      t         : collocation points, shape (N, 1)\n      epochs    : number of gradient descent steps\n      optimizer : tf.keras optimizer (e.g., Adam)\n    \"\"\"\n    for epoch in range(epochs):\n        with tf.GradientTape() as tape:\n            # Compute current loss\n            loss = loss_fn(model, t)\n\n        # Compute gradients w.r.t. all trainable parameters (weights + biases)\n        grads = tape.gradient(loss, model.trainable_variables)\n\n        # Gradient descent update\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        # Simple logging every 500 epochs\n        if epoch % 500 == 0:\n            print(f'Epoch {epoch}: Loss = {loss.numpy():.6e}')\n\n\n# -----------------------------\n# Create model, optimizer, and training data\n# -----------------------------\nmodel = PINN()\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n\n# Training domain: N time points between 0 and 5.\n# np.linspace(... )[:, None] creates an array of shape (N, 1).\nt = tf.convert_to_tensor(np.linspace(0.0, 5.0, 100)[:, None], dtype=tf.float32)\n\n# Train the model\ntrain(model, t, epochs=4000, optimizer=optimizer)\n\n# -----------------------------\n# Prediction on a finer time grid\n# -----------------------------\nt_test = tf.convert_to_tensor(np.linspace(0.0, 5.0, 500)[:, None], dtype=tf.float32)\npred = model(t_test).numpy()       # shape (500, 2)\nx_pred = pred[:, 0]                # shape (500,)\ny_pred = pred[:, 1]                # shape (500,)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analytical solutions\nx_true = 0.5 * np.exp(-t_test) + 0.5 * np.exp(-3 * t_test)\ny_true = -0.5 * np.exp(-t_test) + 0.5 * np.exp(-3 * t_test)\n\n\n# Set up the plot with two subplots side by side\nplt.figure(figsize=(12, 5))\n\n# Plot x(t)\nplt.subplot(1, 2, 1)\nplt.plot(t_test, x_true, label='Analytical x(t)', color='red')\nplt.plot(t_test, x_pred, '--', label='PINNs x(t)', color='blue')\nplt.title(r'PINNs vs Analytical Solution $x(t)$', fontsize=14)\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$x(t)$')\nplt.grid(True)\nplt.legend(fontsize=12, loc=\"upper right\")\n\n\n# Plot y(t)\nplt.subplot(1, 2, 2)\nplt.plot(t_test, y_true, label='Analytical y(t)', color='red')\nplt.plot(t_test, y_pred, '--', label='PINNs y(t)', color='blue')\nplt.title(r'PINNs vs Analytical Solution $y(t)$', fontsize=14)\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$y(t)$')\nplt.grid(True)\nplt.legend(fontsize=12, loc=\"lower right\")\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n## -----------------------------\n### PINN model definition (PyTorch)\n## -----------------------------\nclass PINN(nn.Module):\n    \"\"\"\n    Simple fully-connected network in PyTorch.\n    Input:  t  (N, 1)\n    Output: [x(t), y(t)]  (N, 2)\n    \"\"\"\n    def __init__(self):\n        super(PINN, self).__init__()\n        self.hidden_1 = nn.Linear(1, 64)   # input dim 1 -> 64\n        self.hidden_2 = nn.Linear(64, 64)  # 64 -> 64\n        self.output_3 = nn.Linear(64, 2)   # 64 -> 2 (x,y)\n\n        # Activation function (tanh), we can reuse it\n        self.act = nn.Tanh()\n\n    def forward(self, t):\n        \"\"\"\n        Forward pass.\n        t : tensor of shape (N,1)\n        return : shape (N,2)\n        \"\"\"\n        z = self.act(self.hidden_1(t))\n        z = self.act(self.hidden_2(z))\n        return self.output_3(z)\n\n\n# -----------------------------\n# Loss function (same ODE system)\n# -----------------------------\ndef loss_fn(model, t):\n    \"\"\"\n    Compute PINN loss in PyTorch for:\n        dx/dt + 2x + y = 0\n        dy/dt + x + 2y = 0\n    with x(0) = 1, y(0) = 0.\n    \"\"\"\n\n    # We want derivatives w.r.t. t -> make sure t requires grad\n    t = t.clone().detach().requires_grad_(True)  # keep shape (N,1)\n\n    # Forward pass\n    output = model(t)          # (N,2)\n    x = output[:, 0:1]         # (N,1)\n    y = output[:, 1:2]         # (N,1)\n\n    # Compute dx/dt and dy/dt using autograd.grad\n    # grad_outputs = ones to get element-wise derivative\n    dx_dt = torch.autograd.grad(\n        x, t,\n        grad_outputs=torch.ones_like(x),\n        create_graph=True\n    )[0]                        # shape (N,1)\n\n    dy_dt = torch.autograd.grad(\n        y, t,\n        grad_outputs=torch.ones_like(y),\n        create_graph=True\n    )[0]                        # shape (N,1)\n\n    # ODE residuals\n    res_x = dx_dt + 2.0 * x + y\n    res_y = dy_dt + x + 2.0 * y\n\n    # Mean-squared residuals over all collocation points\n    loss_res_x = torch.mean(res_x**2)\n    loss_res_y = torch.mean(res_y**2)\n\n    # Initial-condition residuals (assume t[0] = 0)\n    # x(0) should be 1,  y(0) should be 0\n    init_loss_x = (x[0] - 1.0)**2\n    init_loss_y = (y[0] - 0.0)**2\n\n    # Total loss\n    loss = loss_res_x + loss_res_y + init_loss_x + init_loss_y\n    return loss\n\n\n# -----------------------------\n# Training loop (PyTorch)\n# -----------------------------\ndef train(model, t, epochs, optimizer):\n    \"\"\"\n    Custom training loop in PyTorch.\n    t : collocation points (N,1) on chosen device.\n    \"\"\"\n    model.train()\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        loss = loss_fn(model, t)\n        loss.backward()            # backprop through network and dx/dt, dy/dt\n        optimizer.step()\n\n        if epoch % 500 == 0:\n            print(f\"[PyTorch] Epoch {epoch}: Loss = {loss.item():.6e}\")\n\n\n# -----------------------------\n# Build model, optimizer, training data\n# -----------------------------\nmodel_torch = PINN().to(device)\noptimizer_torch = optim.Adam(model_torch.parameters(), lr=1e-3)\n\n# Training domain: time points between 0 and 5 (same as TF version)\nt_np = np.linspace(0.0, 5.0, 100).reshape(-1, 1)\nt_torch = torch.tensor(t_np, dtype=torch.float32, device=device)\n\n# Train model\ntrain(model_torch, t_torch, epochs=4000, optimizer=optimizer_torch)\n\n# -----------------------------\n# Prediction\n# -----------------------------\nmodel_torch.eval()\nwith torch.no_grad():\n    t_test_np = np.linspace(0.0, 5.0, 500).reshape(-1, 1)\n    t_test_torch = torch.tensor(t_test_np, dtype=torch.float32, device=device)\n    pred_torch = model_torch(t_test_torch)        # (500,2)\n    x_pred_t  = pred_torch[:, 0].cpu().numpy()    # (500,)\n    y_pred_t  = pred_torch[:, 1].cpu().numpy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t_test = t_test_np.squeeze()   # (500,1) -> (500,)\nx_pred = x_pred_t              \ny_pred = y_pred_t\n\n\n# Analytical solutions\nx_true = 0.5 * np.exp(-t_test) + 0.5 * np.exp(-3 * t_test)\ny_true = -0.5 * np.exp(-t_test) + 0.5 * np.exp(-3 * t_test)\n\n# Set up the plot with two subplots side by side\nplt.figure(figsize=(12, 5))\n\n# Plot x(t)\nplt.subplot(1, 2, 1)\nplt.plot(t_test, x_true, label='Analytical x(t)', color='red')\nplt.plot(t_test, x_pred, '--', label='PINNs x(t)', color='blue')\nplt.title(r'PINNs vs Analytical Solution $x(t)$', fontsize=14)\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$x(t)$')\nplt.grid(True)\nplt.legend(fontsize=12, loc=\"upper right\")\n\n\n# Plot y(t)\nplt.subplot(1, 2, 2)\nplt.plot(t_test, y_true, label='Analytical y(t)', color='red')\nplt.plot(t_test, y_pred, '--', label='PINNs y(t)', color='blue')\nplt.title(r'PINNs vs Analytical Solution $y(t)$', fontsize=14)\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$y(t)$')\nplt.grid(True)\nplt.legend(fontsize=12, loc=\"lower right\")\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}