{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzqsOc0vMHPn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ===========================\n",
        "# 0. Basic configuration\n",
        "# ===========================\n",
        "# Use GPU if available, otherwise fall back to CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Fix random seeds for reproducibility (both PyTorch and NumPy)\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "# Spatial and temporal domain:\n",
        "#   x ∈ [x_min, x_max],  t ∈ [t_min, t_max]\n",
        "x_min, x_max = -5.0, 5.0\n",
        "t_min, t_max = 0.0, np.pi / 2\n",
        "\n",
        "# Create a regular grid in (x, t) for evaluation and plotting\n",
        "#   x: 1D array of spatial points\n",
        "#   t: 1D array of time points\n",
        "x = np.linspace(x_min, x_max, 200)    # Nx = 200 points in x\n",
        "t = np.linspace(t_min, t_max, 100)    # Nt = 100 points in t\n",
        "\n",
        "# Meshgrid for 2D visualization: X, T ∈ R^{Nt×Nx}\n",
        "X, T = np.meshgrid(x, t)\n",
        "\n",
        "# Flatten (X, T) into a list of points (Nt*Nx, 2) with columns [x, t]\n",
        "X_test = np.hstack([X.flatten()[:, None], T.flatten()[:, None]])\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 1. PINN model definition\n",
        "# ===========================\n",
        "class SchrodingerPINN(nn.Module):\n",
        "    \"\"\"\n",
        "    Physics-Informed Neural Network for the nonlinear Schrödinger equation.\n",
        "\n",
        "    The network approximates:\n",
        "        (x, t) -> [u(x,t), v(x,t)]\n",
        "    where:\n",
        "        u(x,t) : real part of the solution\n",
        "        v(x,t) : imaginary part of the solution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            layers: list of layer sizes, e.g. [2, 100, 100, 100, 100, 2]\n",
        "                    layers[0]   = input dimension (x,t)\n",
        "                    layers[-1]  = output dimension (u,v)\n",
        "                    intermediate entries are hidden layer sizes\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        net = []\n",
        "        for i in range(len(layers) - 2):\n",
        "            net.append(nn.Linear(layers[i], layers[i + 1]))\n",
        "            net.append(nn.Tanh())  # Tanh activation in all hidden layers\n",
        "        # Last layer: linear mapping to 2 outputs (u, v), no activation\n",
        "        net.append(nn.Linear(layers[-2], layers[-1]))\n",
        "        self.net = nn.Sequential(*net)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: tensor of shape (N, 2) with columns [x, t]\n",
        "\n",
        "        Returns:\n",
        "            tensor of shape (N, 2): [u_pred, v_pred]\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# Network architecture similar to the DeepXDE example:\n",
        "#   input : 2 (x, t)\n",
        "#   hidden: 4 layers of 100 neurons each\n",
        "#   output: 2 (u, v)\n",
        "layers = [2] + [100] * 4 + [2]\n",
        "model = SchrodingerPINN(layers).to(device)\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 2. Sampling functions\n",
        "# ===========================\n",
        "def sample_interior(N_f):\n",
        "    \"\"\"\n",
        "    Sample collocation points inside the space–time domain for the PDE residual.\n",
        "\n",
        "    Args:\n",
        "        N_f: number of interior points\n",
        "\n",
        "    Returns:\n",
        "        xt_f: tensor of shape (N_f, 2), each row [x, t]\n",
        "    \"\"\"\n",
        "    x_f = np.random.uniform(x_min, x_max, (N_f, 1))\n",
        "    t_f = np.random.uniform(t_min, t_max, (N_f, 1))\n",
        "    xt_f = np.hstack([x_f, t_f])\n",
        "    return torch.tensor(xt_f, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "def sample_initial(N_ic):\n",
        "    \"\"\"\n",
        "    Sample points on the initial time slice t = t_min (initial condition).\n",
        "\n",
        "    Initial condition:\n",
        "        u(x, 0) = 2 / cosh(x)\n",
        "        v(x, 0) = 0\n",
        "\n",
        "    Args:\n",
        "        N_ic: number of initial-condition points\n",
        "\n",
        "    Returns:\n",
        "        xt_ic: tensor of shape (N_ic, 2), points [x, t_min]\n",
        "        u0   : tensor of shape (N_ic, 1), target u(x,0)\n",
        "        v0   : tensor of shape (N_ic, 1), target v(x,0)\n",
        "    \"\"\"\n",
        "    x_ic = np.random.uniform(x_min, x_max, (N_ic, 1))\n",
        "    t_ic = np.full_like(x_ic, t_min)\n",
        "    xt_ic = np.hstack([x_ic, t_ic])\n",
        "\n",
        "    # Exact initial conditions\n",
        "    u0 = 2.0 / np.cosh(x_ic)\n",
        "    v0 = np.zeros_like(x_ic)\n",
        "\n",
        "    xt_ic = torch.tensor(xt_ic, dtype=torch.float32, device=device)\n",
        "    u0 = torch.tensor(u0, dtype=torch.float32, device=device)\n",
        "    v0 = torch.tensor(v0, dtype=torch.float32, device=device)\n",
        "    return xt_ic, u0, v0\n",
        "\n",
        "\n",
        "def sample_boundary(N_bc):\n",
        "    \"\"\"\n",
        "    Sample boundary points at x = x_min and x = x_max for periodic BC enforcement.\n",
        "\n",
        "    For each sampled time t, we create two points:\n",
        "        left  : (x_min, t)\n",
        "        right : (x_max, t)\n",
        "\n",
        "    These will be used to enforce:\n",
        "        u(x_min,t) = u(x_max,t), v(x_min,t) = v(x_max,t)\n",
        "        u_x(x_min,t) = u_x(x_max,t), v_x(x_min,t) = v_x(x_max,t)\n",
        "\n",
        "    Args:\n",
        "        N_bc: number of time samples for the boundary\n",
        "\n",
        "    Returns:\n",
        "        xt_left  : tensor of shape (N_bc, 2) with [x_min, t]\n",
        "        xt_right : tensor of shape (N_bc, 2) with [x_max, t]\n",
        "    \"\"\"\n",
        "    t_bc = np.random.uniform(t_min, t_max, (N_bc, 1))\n",
        "\n",
        "    x_left = np.full_like(t_bc, x_min)\n",
        "    x_right = np.full_like(t_bc, x_max)\n",
        "\n",
        "    xt_left = np.hstack([x_left, t_bc])\n",
        "    xt_right = np.hstack([x_right, t_bc])\n",
        "\n",
        "    xt_left = torch.tensor(xt_left, dtype=torch.float32, device=device)\n",
        "    xt_right = torch.tensor(xt_right, dtype=torch.float32, device=device)\n",
        "    return xt_left, xt_right\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 3. PDE residual (Schrödinger)\n",
        "# ===========================\n",
        "def pde_residual(model, xt):\n",
        "    \"\"\"\n",
        "    Compute the residual of the nonlinear Schrödinger equation:\n",
        "\n",
        "        f_u =   u_t + 0.5 v_xx + (u^2 + v^2) v = 0\n",
        "        f_v = - v_t + 0.5 u_xx + (u^2 + v^2) u = 0\n",
        "\n",
        "    Here:\n",
        "        u(x,t) : real part\n",
        "        v(x,t) : imaginary part\n",
        "\n",
        "    Args:\n",
        "        model: the neural network model\n",
        "        xt   : tensor (N, 2) with [x, t]\n",
        "\n",
        "    Returns:\n",
        "        f_u, f_v: tensors (N,1) representing the PDE residuals for u and v\n",
        "    \"\"\"\n",
        "    # We need gradients w.r.t. x and t, so enable autograd on xt\n",
        "    xt.requires_grad_(True)\n",
        "\n",
        "    # Forward pass through the network: (x,t) -> [u,v]\n",
        "    uv = model(xt)  # shape (N, 2)\n",
        "    u = uv[:, 0:1]  # real part\n",
        "    v = uv[:, 1:2]  # imaginary part\n",
        "\n",
        "    # First derivatives of u: u_x, u_t\n",
        "    grads_u = autograd.grad(\n",
        "        u,\n",
        "        xt,\n",
        "        torch.ones_like(u),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]  # shape (N, 2): [:,0]=∂u/∂x, [:,1]=∂u/∂t\n",
        "    u_x = grads_u[:, 0:1]\n",
        "    u_t = grads_u[:, 1:2]\n",
        "\n",
        "    # First derivatives of v: v_x, v_t\n",
        "    grads_v = autograd.grad(\n",
        "        v,\n",
        "        xt,\n",
        "        torch.ones_like(v),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]  # shape (N, 2): [:,0]=∂v/∂x, [:,1]=∂v/∂t\n",
        "    v_x = grads_v[:, 0:1]\n",
        "    v_t = grads_v[:, 1:2]\n",
        "\n",
        "    # Second derivatives (with respect to x again) for u and v\n",
        "    u_xx = autograd.grad(\n",
        "        u_x,\n",
        "        xt,\n",
        "        torch.ones_like(u_x),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0][:, 0:1]  # take derivative w.r.t x-component\n",
        "    v_xx = autograd.grad(\n",
        "        v_x,\n",
        "        xt,\n",
        "        torch.ones_like(v_x),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0][:, 0:1]\n",
        "\n",
        "    # Nonlinear term |h|^2 = u^2 + v^2\n",
        "    abs_h_sq = u**2 + v**2\n",
        "\n",
        "    # PDE residuals\n",
        "    f_u =   u_t + 0.5 * v_xx + abs_h_sq * v\n",
        "    f_v = - v_t + 0.5 * u_xx + abs_h_sq * u\n",
        "\n",
        "    return f_u, f_v\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 4. Total loss function\n",
        "# ===========================\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "def loss_fn(model, N_f, N_ic, N_bc):\n",
        "    \"\"\"\n",
        "    Build the global PINN loss:\n",
        "        L = L_PDE + α * L_IC + β * L_BC\n",
        "\n",
        "    Where:\n",
        "        - L_PDE   : mean squared residual of the PDE in the interior\n",
        "        - L_IC    : mismatch with initial conditions at t = t_min\n",
        "        - L_BC    : mismatch with periodic boundary conditions at x_min, x_max\n",
        "                    (both function values and first derivatives)\n",
        "\n",
        "    Args:\n",
        "        model: SchrodingerPINN model\n",
        "        N_f  : number of interior points for PDE residual\n",
        "        N_ic : number of points for initial condition\n",
        "        N_bc : number of points for boundary condition\n",
        "\n",
        "    Returns:\n",
        "        total_loss, lpde, lic, lbc_val, lbc_deriv\n",
        "    \"\"\"\n",
        "\n",
        "    # ----- 4.1 Interior (PDE) loss -----\n",
        "    xt_f = sample_interior(N_f)\n",
        "    f_u, f_v = pde_residual(model, xt_f)\n",
        "    loss_pde = mse_loss(f_u, torch.zeros_like(f_u)) + \\\n",
        "               mse_loss(f_v, torch.zeros_like(f_v))\n",
        "\n",
        "    # ----- 4.2 Initial condition loss -----\n",
        "    # Sample points at t = t_min and apply u(x,0)=2/cosh(x), v(x,0)=0\n",
        "    xt_ic, u0, v0 = sample_initial(N_ic)\n",
        "    uv_ic = model(xt_ic)\n",
        "    u_ic = uv_ic[:, 0:1]\n",
        "    v_ic = uv_ic[:, 1:2]\n",
        "\n",
        "    loss_ic = mse_loss(u_ic, u0) + mse_loss(v_ic, v0)\n",
        "\n",
        "    # ----- 4.3 Periodic boundary loss (function values) -----\n",
        "    # Sample points at x_min and x_max for the same times and enforce:\n",
        "    #   u(x_min,t) = u(x_max,t), v(x_min,t) = v(x_max,t)\n",
        "    xt_left, xt_right = sample_boundary(N_bc)\n",
        "    uv_left = model(xt_left)\n",
        "    uv_right = model(xt_right)\n",
        "\n",
        "    u_left = uv_left[:, 0:1]\n",
        "    v_left = uv_left[:, 1:2]\n",
        "    u_right = uv_right[:, 0:1]\n",
        "    v_right = uv_right[:, 1:2]\n",
        "\n",
        "    loss_bc_val = mse_loss(u_left, u_right) + mse_loss(v_left, v_right)\n",
        "\n",
        "    # ----- 4.4 Periodic boundary loss (first derivatives in x) -----\n",
        "    # Enforce periodic derivatives:\n",
        "    #   u_x(x_min,t) = u_x(x_max,t), v_x(x_min,t) = v_x(x_max,t)\n",
        "    xt_left.requires_grad_(True)\n",
        "    xt_right.requires_grad_(True)\n",
        "\n",
        "    uv_left = model(xt_left)\n",
        "    uv_right = model(xt_right)\n",
        "    u_left = uv_left[:, 0:1]\n",
        "    v_left = uv_left[:, 1:2]\n",
        "    u_right = uv_right[:, 0:1]\n",
        "    v_right = uv_right[:, 1:2]\n",
        "\n",
        "    grads_u_left = autograd.grad(\n",
        "        u_left, xt_left, torch.ones_like(u_left),\n",
        "        create_graph=True, retain_graph=True\n",
        "    )[0]\n",
        "    grads_v_left = autograd.grad(\n",
        "        v_left, xt_left, torch.ones_like(v_left),\n",
        "        create_graph=True, retain_graph=True\n",
        "    )[0]\n",
        "    grads_u_right = autograd.grad(\n",
        "        u_right, xt_right, torch.ones_like(u_right),\n",
        "        create_graph=True, retain_graph=True\n",
        "    )[0]\n",
        "    grads_v_right = autograd.grad(\n",
        "        v_right, xt_right, torch.ones_like(v_right),\n",
        "        create_graph=True, retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "    u_x_left = grads_u_left[:, 0:1]\n",
        "    v_x_left = grads_v_left[:, 0:1]\n",
        "    u_x_right = grads_u_right[:, 0:1]\n",
        "    v_x_right = grads_v_right[:, 0:1]\n",
        "\n",
        "    loss_bc_deriv = mse_loss(u_x_left, u_x_right) + mse_loss(v_x_left, v_x_right)\n",
        "\n",
        "    # ----- 4.5 Combine losses with weights -----\n",
        "    # You can tune these weights (10, 1, 1) depending on training behavior.\n",
        "    loss = loss_pde + 10.0 * loss_ic + 1.0 * loss_bc_val + 1.0 * loss_bc_deriv\n",
        "\n",
        "    return loss, loss_pde.item(), loss_ic.item(), loss_bc_val.item(), loss_bc_deriv.item()\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 5. Training loop (Adam)\n",
        "# ===========================\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Number of points for each type of constraint\n",
        "N_f = 5000   # interior collocation points\n",
        "N_ic = 200   # initial-condition points\n",
        "N_bc = 200   # boundary points\n",
        "\n",
        "num_epochs = 5000\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    optimizer.zero_grad()\n",
        "    loss, lpde, lic, lbcv, lbcd = loss_fn(model, N_f, N_ic, N_bc)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training progress every 500 epochs\n",
        "    if epoch % 500 == 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch:5d} | \"\n",
        "            f\"Total: {loss.item():.4e} | \"\n",
        "            f\"PDE: {lpde:.4e} | IC: {lic:.4e} | \"\n",
        "            f\"BC_val: {lbcv:.4e} | BC_deriv: {lbcd:.4e}\"\n",
        "        )\n",
        "\n",
        "# (Optional) You could add an L-BFGS refinement step here if you want.\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 6. Inference on the grid\n",
        "# ===========================\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
        "    pred = model(X_test_tensor).cpu().numpy()   # shape (Nt*Nx, 2)\n",
        "\n",
        "# Reshape predictions back to (Nt, Nx) for plotting:\n",
        "#   T.shape == X.shape == (Nt, Nx) = (100, 200)\n",
        "u_pred = pred[:, 0].reshape(T.shape)  # real part u(x,t)\n",
        "v_pred = pred[:, 1].reshape(T.shape)  # imaginary part v(x,t)\n",
        "\n",
        "# Amplitude (or \"mass density\" in Schrödinger context)\n",
        "h_pred = np.sqrt(u_pred**2 + v_pred**2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4y8L9s4MIp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wVtPWkClMIcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}