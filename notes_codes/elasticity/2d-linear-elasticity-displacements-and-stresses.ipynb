{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T19:09:48.822845Z","iopub.execute_input":"2025-12-03T19:09:48.823126Z","iopub.status.idle":"2025-12-03T19:09:54.397709Z","shell.execute_reply.started":"2025-12-03T19:09:48.823094Z","shell.execute_reply":"2025-12-03T19:09:54.396647Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Hyperparameters\nLR = 1e-3        # learning rate for Adam\nEPOCHS = 10000    # number of training epochs (increase for better results)\nN_HIDDEN = 50    # neurons per hidden layer\nN_LAYERS = 4     # number of hidden layers\nN_PDE = 2000     # number of interior points for PDE\nN_BC = 200       # number of points per boundary segment\n\n# Material and problem parameters\nlambda_ = 1.0\nmu = 0.5\nQ = 4.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------------------------------------\n# Exact (analytical) displacement and stress fields\n#    for the manufactured 2D elasticity solution\n# -----------------------------------------------------------\n\ndef exact_u_x(x, y):\n    \"\"\"\n    Exact x-component of the displacement field u_x(x,y).\n\n    Formula taken from the problem statement:\n        u_x(x,y) = cos(2π x) * sin(π y)\n\n    Inputs:\n      x, y : NumPy arrays or scalars of the same shape\n\n    Output:\n      u_x  : array of same shape as x,y\n    \"\"\"\n    return np.cos(2.0 * np.pi * x) * np.sin(np.pi * y)\n\n\ndef exact_u_y(x, y):\n    \"\"\"\n    Exact y-component of the displacement field u_y(x,y):\n\n        u_y(x,y) = sin(π x) * (Q * y^4 / 4)\n\n    Q is a loading parameter defined globally (e.g., Q = 4).\n    \"\"\"\n    return np.sin(np.pi * x) * (Q * (y**4) / 4.0)\n\n\ndef exact_sigma_xx(x, y):\n    \"\"\"\n    Exact normal stress σ_xx(x,y) derived from exact displacements\n    via the linear elasticity constitutive law:\n\n        σ_xx = (λ + 2μ) * ε_xx + λ * ε_yy\n\n    where\n        ε_xx = ∂u_x/∂x\n        ε_yy = ∂u_y/∂y\n\n    We explicitly compute u_x_x and u_y_y using the analytical u_x, u_y.\n    \"\"\"\n    # ε_xx = ∂u_x/∂x\n    # u_x = cos(2πx) sin(πy) → ∂/∂x = -2π sin(2πx) sin(πy)\n    u_x_x = -2.0 * np.pi * np.sin(2.0 * np.pi * x) * np.sin(np.pi * y)\n\n    # ε_yy = ∂u_y/∂y\n    # u_y = sin(πx) * (Q y^4 / 4) → ∂/∂y = sin(πx) * Q y^3\n    u_y_y = Q * np.sin(np.pi * x) * (y**3)\n\n    # σ_xx = (λ + 2μ) * ε_xx + λ * ε_yy\n    return (lambda_ + 2 * mu) * u_x_x + lambda_ * u_y_y\n\n\ndef exact_sigma_yy(x, y):\n    \"\"\"\n    Exact normal stress σ_yy(x,y):\n\n        σ_yy = (λ + 2μ) * ε_yy + λ * ε_xx\n    using the same ε_xx, ε_yy as above.\n    \"\"\"\n    u_x_x = -2.0 * np.pi * np.sin(2.0 * np.pi * x) * np.sin(np.pi * y)\n    u_y_y = Q * np.sin(np.pi * x) * (y**3)\n    return (lambda_ + 2 * mu) * u_y_y + lambda_ * u_x_x\n\n\ndef exact_sigma_xy(x, y):\n    \"\"\"\n    Exact shear stress σ_xy(x,y):\n\n        σ_xy = μ * (ε_xy + ε_yx)\n    with\n        ε_xy = 1/2 (∂u_x/∂y + ∂u_y/∂x)\n    but in this manufactured solution an equivalent closed form\n    expression is already given:\n\n        σ_xy = μ [ π cos(2πx) cos(πy)\n                   + π (Q y^4 / 4) cos(πx) ]\n\n    Inputs:\n      x, y : NumPy arrays or scalars\n\n    Output:\n      σ_xy : array of same shape as x,y\n    \"\"\"\n    return mu * (\n        np.pi * np.cos(2.0 * np.pi * x) * np.cos(np.pi * y)\n        + np.pi * (Q * (y**4) / 4.0) * np.cos(np.pi * x)\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(\"cpu\")\n\n\nclass PINN(nn.Module):\n    def __init__(self, layers, neurons):\n        \"\"\"\n        Physics-Informed Neural Network for 2D linear elasticity.\n\n        Input  : (x, y)  ∈ [0,1]²  (spatial coordinates)\n        Output : [u_x, u_y, sigma_xx, sigma_yy, sigma_xy]\n                 = 5 fields predicted at each (x,y).\n\n        layers  : number of hidden layers (depth)\n        neurons : number of neurons per hidden layer (width)\n        \"\"\"\n        super().__init__()\n\n        # First fully connected layer: maps 2D input -> hidden features\n        self.input_layer = nn.Linear(2, neurons)\n\n        # Hidden layers: each maps (neurons -> neurons)\n        # nn.ModuleList is used so that PyTorch knows these are trainable submodules.\n        self.hidden = nn.ModuleList(\n            [nn.Linear(neurons, neurons) for _ in range(layers - 1)]\n        )\n\n        # Output layer: maps hidden features -> 5 outputs (displacements and stresses)\n        #   out[:,0] = u_x\n        #   out[:,1] = u_y\n        #   out[:,2] = sigma_xx\n        #   out[:,3] = sigma_yy\n        #   out[:,4] = sigma_xy\n        self.output_layer = nn.Linear(neurons, 5)         # 5 outputs '(neuron, 5)'\n\n        # Nonlinear activation used after each linear layer (except last).\n        # Tanh is smooth and differentiable, which is useful when computing\n        # many derivatives via autograd for PDE residuals.\n        self.activation = nn.Tanh()\n        \n        # Weight initialization: Xavier normal tailored for tanh\n        # This keeps variance of activations relatively stable across layers.\n        for m in self.hidden:\n            nn.init.xavier_normal_(m.weight, gain=nn.init.calculate_gain('tanh'))\n        nn.init.xavier_normal_(self.input_layer.weight,\n                               gain=nn.init.calculate_gain('tanh'))\n        # Output layer typically uses gain=1 (linear output)\n        nn.init.xavier_normal_(self.output_layer.weight)\n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass.\n\n        x : tensor of shape (N, 2) where each row is (x_i, y_i)\n\n        Returns:\n          out : tensor of shape (N, 5)\n                columns = [u_x, u_y, sigma_xx, sigma_yy, sigma_xy]\n        \"\"\"\n        # Apply first linear layer + activation\n        z = self.activation(self.input_layer(x))\n\n        # Propagate through hidden layers, each followed by tanh\n        for m in self.hidden:\n            z = self.activation(m(z))\n\n        # Final linear layer (no activation) to get 5 physical outputs\n        return self.output_layer(z)\n\n\n# Instantiate the model with chosen depth/width and move to CPU/GPU device\nmodel = PINN(N_LAYERS, N_HIDDEN).to(device)\n\n\n\n# -------------------------------------------------------------------\n# Body force (source terms) f_x(x,y), f_y(x,y) used in the equilibrium\n# equations: ∂σ_xx/∂x + ∂σ_xy/∂y + f_x = 0, etc.\n# These are manufactured from the known exact solution.\n# -------------------------------------------------------------------\ndef f_x_fun(x, y):\n    term1 = lambda_ * (\n        4.0 * np.pi**2 * torch.cos(2.0 * np.pi * x) * torch.sin(np.pi * y)\n        - np.pi * torch.cos(np.pi * x) * Q * (y**3)\n    )\n    term2 = mu * (\n        9.0 * np.pi**2 * torch.cos(2.0 * np.pi * x) * torch.sin(np.pi * y)\n        - np.pi * torch.cos(np.pi * x) * Q * (y**3)\n    )\n    return term1 + term2\n\n\ndef f_y_fun(x, y):\n    term1 = lambda_ * (\n        -3.0 * torch.sin(np.pi * x) * Q * (y**2)\n        + 2.0 * np.pi**2 * torch.sin(2.0 * np.pi * x) * torch.cos(np.pi * y)\n    )\n    term2 = mu * (\n        -6.0 * torch.sin(np.pi * x) * Q * (y**2)\n        + 2.0 * np.pi**2 * torch.sin(2.0 * np.pi * x) * torch.cos(np.pi * y)\n        + (np.pi**2) * torch.sin(np.pi * x) * Q * (y**4) / 4.0\n    )\n    return term1 + term2\n\n\n\n# -------------------------------------------------------------------\n# Sampling of training points: interior (for PDE) and boundaries (for BC)\n# -------------------------------------------------------------------\n\n# Interior PDE collocation points: N_PDE points uniformly in [0,1]²\nxy_pde = np.random.rand(N_PDE, 2)                  # shape (N_PDE, 2), each row (x,y)\nxy_pde_torch = torch.tensor(xy_pde, dtype=torch.float32).to(device)\n\n# Boundary parametric points along each side of the unit square:\n# y = 0, y = 1, x = 0, x = 1\n\n# y = 0: x ∈ [0,1]\nbc_y0 = np.linspace(0, 1, N_BC)\nxy_bc_y0 = np.column_stack((bc_y0, np.zeros_like(bc_y0)))  # (x, 0)\n\n# y = 1: x ∈ [0,1]\nbc_y1 = np.linspace(0, 1, N_BC)\nxy_bc_y1 = np.column_stack((bc_y1, np.ones_like(bc_y1)))   # (x, 1)\n\n# x = 0: y ∈ [0,1]\nbc_x0 = np.linspace(0, 1, N_BC)\nxy_bc_x0 = np.column_stack((np.zeros_like(bc_x0), bc_x0))  # (0, y)\n\n# x = 1: y ∈ [0,1]\nbc_x1 = np.linspace(0, 1, N_BC)\nxy_bc_x1 = np.column_stack((np.ones_like(bc_x1), bc_x1))   # (1, y)\n\n# Convert all boundary sets to torch tensors on the device\nxy_bc_y0_torch = torch.tensor(xy_bc_y0, dtype=torch.float32).to(device)\nxy_bc_y1_torch = torch.tensor(xy_bc_y1, dtype=torch.float32).to(device)\nxy_bc_x0_torch = torch.tensor(xy_bc_x0, dtype=torch.float32).to(device)\nxy_bc_x1_torch = torch.tensor(xy_bc_x1, dtype=torch.float32).to(device)\n\n\n\ndef loss_pde(xy):\n    \"\"\"\n    Compute PDE residual loss over interior points.\n\n    This PINN enforces the *system* of 5 equations:\n\n      1) ∂σ_xx/∂x + ∂σ_xy/∂y + f_x = 0       (force balance in x)\n      2) ∂σ_xy/∂x + ∂σ_yy/∂y + f_y = 0       (force balance in y)\n      3) σ_xx - [ (λ+2μ)*u_xx + λ*u_yy ] = 0  (constitutive eq. for σ_xx)\n      4) σ_yy - [ (λ+2μ)*u_yy + λ*u_xx ] = 0  (constitutive eq. for σ_yy)\n      5) σ_xy - μ (u_x_y + u_y_x) = 0         (constitutive eq. for σ_xy)\n\n    So stresses are not only *derived* from u, but also *output* by the network\n    and constrained to satisfy these constitutive relations.\n    \"\"\"\n    # Make a copy of xy that is detached from any previous graph,\n    # and enable gradient tracking so we can differentiate w.r.t. x,y.\n    xy_ = xy.clone().detach().requires_grad_(True)\n    x = xy_[:, 0:1]   # (N,1) x-coordinates\n    y = xy_[:, 1:2]   # (N,1) y-coordinates\n    \n    # Forward pass through the network\n    out = model(torch.cat((x, y), dim=1))  # (N,5)\n    u_x = out[:, 0:1]  # displacement component u_x(x,y)\n    u_y = out[:, 1:2]  # displacement component u_y(x,y)\n    sxx = out[:, 2:3]  # predicted σ_xx(x,y)\n    syy = out[:, 3:4]  # predicted σ_yy(x,y)\n    sxy = out[:, 4:5]  # predicted σ_xy(x,y)\n\n    \n    # -------- Equilibrium equations (1) and (2) --------\n    # ∂σ_xx/∂x, ∂σ_xy/∂y, ∂σ_xy/∂x, ∂σ_yy/∂y\n    sxx_x = torch.autograd.grad(sxx, x, torch.ones_like(sxx),\n                                create_graph=True)[0]\n    sxy_y = torch.autograd.grad(sxy, y, torch.ones_like(sxy),\n                                create_graph=True)[0]\n    sxy_x = torch.autograd.grad(sxy, x, torch.ones_like(sxy),\n                                create_graph=True)[0]\n    syy_y = torch.autograd.grad(syy, y, torch.ones_like(syy),\n                                create_graph=True)[0]\n\n    # Body forces at (x,y)\n    fx = f_x_fun(x, y)\n    fy = f_y_fun(x, y)\n\n    ## Residuals of momentum balance\n    # Resiudal 1 = ∂σ_xx/∂x + ∂σ_xy/∂y + f_x\n    eq1 = sxx_x + sxy_y + fx  # should be ≈ 0\n    # Resiudal 2 = ∂σ_yy/∂y + ∂σ_yx/∂x + f_y\n    eq2 = sxy_x + syy_y + fy  # should be ≈ 0 \n\n    \n    # -------- Constitutive equations (3), (4), (5) --------\n    # First derivatives of displacements\n    u_x_x = torch.autograd.grad(u_x, x, torch.ones_like(u_x),\n                                create_graph=True)[0]\n    u_y_y = torch.autograd.grad(u_y, y, torch.ones_like(u_y),\n                                create_graph=True)[0]\n    u_x_y = torch.autograd.grad(u_x, y, torch.ones_like(u_x),\n                                create_graph=True)[0]\n    u_y_x = torch.autograd.grad(u_y, x, torch.ones_like(u_y),\n                                create_graph=True)[0]\n\n    ## Constitutive residuals:\n    #   Residual 3) σ_xx - ((λ+2μ) u_xx + λ u_yy) = 0  ~ Residual 3\n    eq3 = sxx - ((lambda_ + 2 * mu) * u_x_x + lambda_ * u_y_y)\n    \n    #   Residual 4) σ_yy - ((λ+2μ) u_yy + λ u_xx) = 0  ~ Residual 4\n    eq4 = syy - ((lambda_ + 2 * mu) * u_y_y + lambda_ * u_x_x)\n\n    #   Residual 5) σ_xy - μ (u_xy + u_yx) = 0         ~ Residual 5    \n    eq5 = sxy - mu * (u_x_y + u_y_x)\n\n    # Total PDE loss = mean squared residual of all 5 equations\n    return torch.mean(eq1**2 + eq2**2 + eq3**2 + eq4**2 + eq5**2)\n\n\n\ndef loss_bc():\n    \"\"\"\n    Boundary-condition loss.\n\n    We enforce displacement and traction conditions by using the\n    network outputs {u_x, u_y, σ_xx, σ_yy, σ_xy} at boundary points.\n\n    Conditions:\n\n      y = 0 : u_x = 0, u_y = 0\n      y = 1 : u_x = 0, σ_yy = (λ+2μ) Q sin(π x)\n      x = 0 : u_y = 0, σ_xx = 0\n      x = 1 : u_y = 0, σ_xx = 0\n    \"\"\"\n    loss_val = 0.0\n    \n    # ---------------- y = 0 : u_x = 0, u_y = 0 ----------------\n    x0 = xy_bc_y0_torch[:, 0:1].clone().detach().requires_grad_(True)\n    y0 = xy_bc_y0_torch[:, 1:2].clone().detach().requires_grad_(True)\n    out_y0 = model(torch.cat((x0, y0), dim=1))  # (N_BC,5)\n    # Penalize non-zero displacement\n    loss_val += torch.mean(out_y0[:, 0:1]**2)  # u_x -> 0\n    loss_val += torch.mean(out_y0[:, 1:2]**2)  # u_y -> 0\n    \n    # ---------------- y = 1 : u_x = 0, σ_yy = (λ+2μ) Q sin(π x) ----------------\n    x1 = xy_bc_y1_torch[:, 0:1].clone().detach().requires_grad_(True)\n    y1 = xy_bc_y1_torch[:, 1:2].clone().detach().requires_grad_(True)\n    out_y1 = model(torch.cat((x1, y1), dim=1))\n    # u_x(x,1) -> 0\n    loss_val += torch.mean(out_y1[:, 0:1]**2)\n    # σ_yy at y=1\n    syy_1 = out_y1[:, 3:4]\n    syy_target = (lambda_ + 2 * mu) * Q * torch.sin(np.pi * x1)\n    loss_val += torch.mean((syy_1 - syy_target)**2)\n    \n    # ---------------- x = 0 : u_y = 0, σ_xx = 0 ----------------\n    x0_ = xy_bc_x0_torch[:, 0:1].clone().detach().requires_grad_(True)\n    y0_ = xy_bc_x0_torch[:, 1:2].clone().detach().requires_grad_(True)\n    out_x0 = model(torch.cat((x0_, y0_), dim=1))\n    # u_y(0,y) -> 0\n    loss_val += torch.mean(out_x0[:, 1:2]**2)\n    # σ_xx(0,y) -> 0  (component index 2)\n    loss_val += torch.mean(out_x0[:, 2:3]**2)\n    \n    # ---------------- x = 1 : u_y = 0, σ_xx = 0 ----------------\n    x1_ = xy_bc_x1_torch[:, 0:1].clone().detach().requires_grad_(True)\n    y1_ = xy_bc_x1_torch[:, 1:2].clone().detach().requires_grad_(True)\n    out_x1 = model(torch.cat((x1_, y1_), dim=1))\n    # u_y(1,y) -> 0\n    loss_val += torch.mean(out_x1[:, 1:2]**2)\n    # σ_xx(1,y) -> 0\n    loss_val += torch.mean(out_x1[:, 2:3]**2)\n\n    return loss_val\n\n\n\n# -------------------------------------------------------------------\n# Training loop: minimize PDE residual + BC residual\n# -------------------------------------------------------------------\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\nfor ep in range(EPOCHS):\n    optimizer.zero_grad()\n\n    # Interior (PDE) residual loss\n    loss_interior = loss_pde(xy_pde_torch)\n\n    # Boundary condition loss on all four sides\n    loss_bound = loss_bc()\n\n    # Total loss (here simple sum, weights could be tuned)\n    loss_total = loss_interior + loss_bound\n\n    # Backpropagation\n    loss_total.backward()\n    optimizer.step()\n\n    if ep % 500 == 0:\n        print(\n            f\"Epoch {ep:5d} | \"\n            f\"PDE Loss: {loss_interior.item():.4e} | \"\n            f\"BC Loss: {loss_bound.item():.4e} | \"\n            f\"Total: {loss_total.item():.4e}\"\n        )\n\nprint(\"Training finished.\")\n\n\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\nfor ep in range(EPOCHS):\n    optimizer.zero_grad()\n    loss_interior = loss_pde(xy_pde_torch)\n    loss_bound = loss_bc()\n    loss_total = loss_interior + loss_bound\n    loss_total.backward()\n    optimizer.step()\n\n    if ep % 500 == 0:\n        print(f\"Epoch {ep:5d} | PDE Loss: {loss_interior.item():.4e} | BC Loss: {loss_bound.item():.4e} | Total: {loss_total.item():.4e}\")\n\nprint(\"Training finished.\")\n\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------------------------------------\n# Build evaluation grid and run the trained PINN\n# -----------------------------------------------------------\n\n# Stack the grid coordinates XX, YY into a list of (x, y) pairs.\n#   XX, YY have shape (ny, nx).\n#   XX.ravel(), YY.ravel() → 1D arrays of length ny*nx.\n#   np.column_stack → array of shape (ny*nx, 2): [(x_1, y_1), ..., (x_N, y_N)].\nXY_plot = np.column_stack((XX.ravel(), YY.ravel()))\n\n# Convert the evaluation points into a torch tensor on the correct device.\nXY_torch = torch.tensor(XY_plot, dtype=torch.float32).to(device)\n\n# Disable gradient tracking during inference to save memory and computation.\nwith torch.no_grad():\n    # Forward pass through the PINN at all evaluation points.\n    # Output shape: (ny*nx, 5) = [u_x, u_y, sxx, syy, sxy].\n    pred = model(XY_torch).cpu().numpy()  # move to CPU and convert to NumPy\n\n# Split the 5 outputs and reshape back to (ny, nx) grids for contour plots.\nu_x_pred = pred[:, 0].reshape(ny, nx)\nu_y_pred = pred[:, 1].reshape(ny, nx)\nsxx_pred = pred[:, 2].reshape(ny, nx)\nsyy_pred = pred[:, 3].reshape(ny, nx)\nsxy_pred = pred[:, 4].reshape(ny, nx)\n\n# Exact (analytical) solutions on the same grid (XX, YY).\n# These functions should return arrays with shape (ny, nx).\nu_x_ex = exact_u_x(XX, YY)\nu_y_ex = exact_u_y(XX, YY)\nsigma_xx_exact = exact_sigma_xx(XX, YY)\nsigma_yy_exact = exact_sigma_yy(XX, YY)\nsigma_xy_exact = exact_sigma_xy(XX, YY)\n\n\n# -----------------------------------------------------------\n# Compare u_x : PINN vs exact solution\n# -----------------------------------------------------------\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: PINN-predicted displacement u_x(x,y)\ncont1 = axs[0].contourf(XX, YY, u_x_pred, levels=30)\nfig.colorbar(cont1, ax=axs[0])\naxs[0].set_title(\"PINN predicted $u_x$\")\naxs[0].set_xlabel(\"x\")\naxs[0].set_ylabel(\"y\")\n\n# Right: exact displacement u_x(x,y)\ncont2 = axs[1].contourf(XX, YY, u_x_ex, levels=30)\nfig.colorbar(cont2, ax=axs[1])\naxs[1].set_title(\"Exact $u_x$\")\naxs[1].set_xlabel(\"x\")\naxs[1].set_ylabel(\"y\")\n\nplt.tight_layout()\nplt.show()\n\n\n# -----------------------------------------------------------\n# Compare u_y : PINN vs exact solution\n# -----------------------------------------------------------\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: PINN-predicted displacement u_y(x,y)\ncont3 = axs[0].contourf(XX, YY, u_y_pred, levels=30)\nfig.colorbar(cont3, ax=axs[0])\naxs[0].set_title(\"PINN predicted $u_y$\")\naxs[0].set_xlabel(\"x\")\naxs[0].set_ylabel(\"y\")\n\n# Right: exact displacement u_y(x,y)\ncont4 = axs[1].contourf(XX, YY, u_y_ex, levels=30)\nfig.colorbar(cont4, ax=axs[1])\naxs[1].set_title(\"Exact $u_y$\")\naxs[1].set_xlabel(\"x\")\naxs[1].set_ylabel(\"y\")\n\nplt.tight_layout()\nplt.show()\n\n\n# -----------------------------------------------------------\n# Compare σ_xx : PINN vs exact solution\n# -----------------------------------------------------------\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: PINN-predicted normal stress σ_xx(x,y)\ncont5 = axs[0].contourf(XX, YY, sxx_pred, levels=30)\nfig.colorbar(cont5, ax=axs[0])\naxs[0].set_title(\"PINN predicted $\\sigma_{xx}$\")\naxs[0].set_xlabel(\"x\")\naxs[0].set_ylabel(\"y\")\n\n# Right: exact σ_xx(x,y)\ncont6 = axs[1].contourf(XX, YY, sigma_xx_exact, levels=30)\nfig.colorbar(cont6, ax=axs[1])\naxs[1].set_title(\"Exact $\\sigma_{xx}$\")\naxs[1].set_xlabel(\"x\")\naxs[1].set_ylabel(\"y\")\n\nplt.tight_layout()\nplt.show()\n\n\n# -----------------------------------------------------------\n# Compare σ_yy : PINN vs exact solution\n# -----------------------------------------------------------\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: PINN-predicted normal stress σ_yy(x,y)\ncont7 = axs[0].contourf(XX, YY, syy_pred, levels=30)\nfig.colorbar(cont7, ax=axs[0])\naxs[0].set_title(\"PINN predicted $\\sigma_{yy}$\")\naxs[0].set_xlabel(\"x\")\naxs[0].set_ylabel(\"y\")\n\n# Right: exact σ_yy(x,y)\ncont8 = axs[1].contourf(XX, YY, sigma_yy_exact, levels=30)\nfig.colorbar(cont8, ax=axs[1])\naxs[1].set_title(\"Exact $\\sigma_{yy}$\")\naxs[1].set_xlabel(\"x\")\naxs[1].set_ylabel(\"y\")\n\nplt.tight_layout()\nplt.show()\n\n\n# -----------------------------------------------------------\n# Compare σ_xy : PINN vs exact solution\n# -----------------------------------------------------------\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: PINN-predicted shear stress σ_xy(x,y)\ncont9 = axs[0].contourf(XX, YY, sxy_pred, levels=30)\nfig.colorbar(cont9, ax=axs[0])\naxs[0].set_title(\"PINN predicted $\\sigma_{xy}$\")\naxs[0].set_xlabel(\"x\")\naxs[0].set_ylabel(\"y\")\n\n# Right: exact σ_xy(x,y)\ncont10 = axs[1].contourf(XX, YY, sigma_xy_exact, levels=30)\nfig.colorbar(cont10, ax=axs[1])\naxs[1].set_title(\"Exact $\\sigma_{xy}$\")\naxs[1].set_xlabel(\"x\")\naxs[1].set_ylabel(\"y\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}