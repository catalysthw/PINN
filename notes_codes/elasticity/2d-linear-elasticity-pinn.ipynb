{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:02:16.585802Z","iopub.execute_input":"2025-12-03T18:02:16.586080Z","iopub.status.idle":"2025-12-03T18:02:22.074378Z","shell.execute_reply.started":"2025-12-03T18:02:16.586056Z","shell.execute_reply":"2025-12-03T18:02:22.073310Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Hyperparameters\nLR = 1e-3        # learning rate for Adam\nEPOCHS = 5000    # number of training epochs (increase for better results)\nN_HIDDEN = 50    # neurons per hidden layer\nN_LAYERS = 4     # number of hidden layers\nN_PDE = 2000     # number of interior points for PDE\nN_BC = 200       # number of points per boundary segment\n\n# Material and problem parameters\nlambda_ = 1.0\nmu = 0.5\nQ = 4.0\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(\"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nAs PINN DL engineer, what are the points to adjust in this network and training:\n\n1) N_LAYERS, N_HIDDEN → Network capacity adjustment\n2) N_PDE, N_BC → collocation / boundary sampling density\n3) loss = w_pde * loss_pde + w_bc * loss_bc: Weight tuninig\n4) activation(tanh → swish, sine, etc.) function switch\n5) initialization method change, optimizer/learning rate scheduel adjustment\n'''\n\n\n# ============================================================\n# 1. Neural network model: PINN for 2D elasticity\n#    Input:  (x, y)\n#    Output: (u_x, u_y)  -> displacement field components\n# ============================================================\n\nclass PINN(nn.Module):\n    def __init__(self, layers, neurons):\n        \"\"\"\n        layers  : number of hidden layers (depth)\n        neurons : number of neurons per hidden layer (width)\n\n        Network architecture (schematic):\n          Input (x, y) ∈ R^2\n            → Linear(2 -> neurons) + Tanh\n            → [layers-1] × [Linear(neurons -> neurons) + Tanh]\n            → Linear(neurons -> 2)  (outputs: [u_x, u_y])\n        \"\"\"\n        super().__init__()\n\n        # First fully-connected layer: maps 2D input (x,y) to 'neurons'-dim hidden features.\n        self.input_layer = nn.Linear(2, neurons)\n\n        # List of hidden layers, each Linear(neurons -> neurons).\n        # nn.ModuleList is used so that PyTorch can register all layers'\n        # parameters correctly (so they are updated during training).\n        self.hidden = nn.ModuleList([\n            nn.Linear(neurons, neurons) for _ in range(layers - 1)\n        ])\n\n        # Final output layer: maps hidden features → 2 outputs.\n        # Here, the two outputs correspond to displacement components:\n        # u_x(x,y) and u_y(x,y).\n        self.output_layer = nn.Linear(neurons, 2)\n\n        # Activation function used after each Linear layer (except the last).\n        # Tanh is commonly chosen in PINNs because it is smooth and\n        # infinitely differentiable (good for higher-order PDE derivatives).\n        self.activation = nn.Tanh()\n        \n        # --------------------------------------------------------\n        # Weight initialization: Xavier normal, adapted for tanh.\n        # This helps keep signal/gradient magnitudes stable\n        # across layers, improving training stability.\n        # --------------------------------------------------------\n        for m in self.hidden:\n            nn.init.xavier_normal_(\n                m.weight,\n                gain=nn.init.calculate_gain('tanh')\n            )\n        nn.init.xavier_normal_(\n            self.input_layer.weight,\n            gain=nn.init.calculate_gain('tanh')\n        )\n        # Output layer often does not use an activation; we can use\n        # Xavier normal with default gain (=1).\n        nn.init.xavier_normal_(self.output_layer.weight)\n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the network.\n\n        Input:\n          x : tensor of shape (N, 2) where each row is (x_i, y_i).\n\n        Steps:\n          1) Apply input_layer + tanh.\n          2) Pass through each hidden layer with tanh.\n          3) Apply final output_layer (no activation).\n        \n        Output:\n          tensor of shape (N, 2), each row [u_x(x_i, y_i), u_y(x_i, y_i)].\n        \"\"\"\n        # First layer + activation\n        z = self.activation(self.input_layer(x))\n\n        # Propagate through all hidden layers, applying tanh each time\n        for m in self.hidden:\n            z = self.activation(m(z))\n\n        # Final linear layer outputs 2D displacement vector at each point\n        return self.output_layer(z)\n\n\n# Instantiate the model using chosen hyperparameters:\n#   N_LAYERS : number of hidden layers\n#   N_HIDDEN : number of neurons per hidden layer\n#   device   : 'cpu' or 'cuda'\nmodel = PINN(N_LAYERS, N_HIDDEN).to(device)\n\n\n\n# ============================================================\n# 2. Boundary force (body force / source terms) for elasticity PDE\n#    These are the right-hand side terms f_x(x,y), f_y(x,y)\n#    in the equilibrium equations:\n#\n#   ∂σ_xx/∂x + ∂σ_xy/∂y + f_x = 0\n#   ∂σ_xy/∂x + ∂σ_yy/∂y + f_y = 0\n#   \n#   lambda_, mu : Lamé parameters (material constants)\n#   Q          : some loading parameter\n# ============================================================\n\n# Body forces\ndef f_x_fun(x, y):\n    \"\"\"\n    Compute x-component of the body force f_x(x,y).\n\n    term1, term2 correspond to contributions from lambda_ and mu\n    in a manufactured (constructed) forcing function that is\n    consistent with a chosen analytical displacement field.\n    \"\"\"\n    term1 = lambda_ * (\n        4.0 * np.pi**2 * torch.cos(2.0 * np.pi * x) * torch.sin(np.pi * y)\n        - np.pi * torch.cos(np.pi * x) * Q * (y**3)\n    )\n    term2 = mu * (\n        9.0 * np.pi**2 * torch.cos(2.0 * np.pi * x) * torch.sin(np.pi * y)\n        - np.pi * torch.cos(np.pi * x) * Q * (y**3)\n    )\n    return term1 + term2\n\n\ndef f_y_fun(x, y):\n    \"\"\"\n    Compute y-component of the body force f_y(x,y).\n\n    Again, lambda_ and mu terms are grouped as term1 and term2, and the\n    structure is chosen so that the PDE has a known analytical solution\n    (manufactured solution technique).\n    \"\"\"\n    term1 = lambda_ * (\n        -3.0 * torch.sin(np.pi * x) * Q * (y**2)\n        + 2.0 * np.pi**2 * torch.sin(2.0 * np.pi * x) * torch.cos(np.pi * y)\n    )\n    term2 = mu * (\n        -6.0 * torch.sin(np.pi * x) * Q * (y**2)\n        + 2.0 * np.pi**2 * torch.sin(2.0 * np.pi * x) * torch.cos(np.pi * y)\n        + (np.pi**2) * torch.sin(np.pi * x) * Q * (y**4) / 4.0\n    )\n    return term1 + term2\n\n\n\n# ============================================================\n# 3. Sampling training points: interior (PDE) + boundary (BC)\n# ============================================================\n\n## Interior PDE collocation points\n# Randomly sample N_PDE points in the unit square [0,1]^2.\n# These are the points where we enforce the PDE residual (equilibrium equations).\nxy_pde = np.random.rand(N_PDE, 2)  # shape: (N_PDE, 2) with entries in [0,1]\nxy_pde_torch = torch.tensor(xy_pde, dtype=torch.float32).to(device)\n\n\n# ------------------------------------------------------------\n# Boundary points: each side of the unit square\n#   y = 0, y = 1, x = 0, x = 1\n# ------------------------------------------------------------\n\n# y = 0 boundary: x ∈ [0,1]\nbc_y0 = np.linspace(0, 1, N_BC)                   # param along x\nxy_bc_y0 = np.column_stack((bc_y0, np.zeros_like(bc_y0)))  # (x, 0)\n\n# y = 1 boundary: x ∈ [0,1]\nbc_y1 = np.linspace(0, 1, N_BC)\nxy_bc_y1 = np.column_stack((bc_y1, np.ones_like(bc_y1)))   # (x, 1)\n\n# x = 0 boundary: y ∈ [0,1]\nbc_x0 = np.linspace(0, 1, N_BC)\nxy_bc_x0 = np.column_stack((np.zeros_like(bc_x0), bc_x0))  # (0, y)\n\n# x = 1 boundary: y ∈ [0,1]\nbc_x1 = np.linspace(0, 1, N_BC)\nxy_bc_x1 = np.column_stack((np.ones_like(bc_x1), bc_x1))   # (1, y)\n\n# Convert all boundary point arrays to PyTorch tensors on the chosen device\nxy_bc_y0_torch = torch.tensor(xy_bc_y0, dtype=torch.float32).to(device)\nxy_bc_y1_torch = torch.tensor(xy_bc_y1, dtype=torch.float32).to(device)\nxy_bc_x0_torch = torch.tensor(xy_bc_x0, dtype=torch.float32).to(device)\nxy_bc_x1_torch = torch.tensor(xy_bc_x1, dtype=torch.float32).to(device)\n\n\n\n\n\n# ============================================================\n# 4. PDE residual loss L_PDE\n#    Implements the elasticity equilibrium equations:\n#\n#   ∂σ_xx/∂x + ∂σ_xy/∂y + f_x = 0\n#   ∂σ_xy/∂x + ∂σ_yy/∂y + f_y = 0\n#\n#   where σ_xx, σ_yy, σ_xy are stresses derived from displacement u.\n# ============================================================\n\ndef loss_pde(xy):\n    \"\"\"\n    Compute the mean-squared PDE residual over a batch of interior points.\n\n    Input:\n      xy : tensor of shape (N, 2) with coordinates (x_i, y_i).\n\n    Steps:\n      1) Enable autograd on xy to differentiate displacement w.r.t x,y.\n      2) Compute displacement u(x,y) = [u_x, u_y] from the PINN.\n      3) Compute spatial derivatives u_{x,x}, u_{x,y}, u_{y,x}, u_{y,y}.\n      4) Build stress components σ_xx, σ_yy, σ_xy via linear elasticity.\n      5) Take divergence of stress + body force:\n         r1 = ∂σ_xx/∂x + ∂σ_xy/∂y + f_x\n         r2 = ∂σ_xy/∂x + ∂σ_yy/∂y + f_y\n      6) Return mean squared residual: E[r1^2 + r2^2].\n    \"\"\"\n    # Clone/detach so we don't accidentally backprop through earlier graph;\n    # requires_grad_(True) so that autograd tracks derivatives w.r.t x,y.\n    xy_ = xy.clone().detach().requires_grad_(True)\n    x = xy_[:, 0:1]  # shape: (N,1)\n    y = xy_[:, 1:2]  # shape: (N,1)\n\n    # Forward pass: displacement field u(x,y) from PINN\n    # We concatenate x and y along feature dimension to get shape (N,2).\n    u = model(torch.cat((x, y), dim=1))  # shape: (N,2)\n    u_x = u[:, 0:1]  # displacement component in x-direction\n    u_y = u[:, 1:2]  # displacement component in y-direction\n\n    # ----------------------\n    # Compute first derivatives of displacement\n    # ----------------------\n\n    # ∂u_x/∂x ~ ε_xx (strain component)\n    grad_u_x_x = torch.autograd.grad(\n        u_x, x,\n        torch.ones_like(u_x),\n        create_graph=True)[0]\n\n    # ∂u_x/∂y ~ ε_xy (strain component)\n    grad_u_x_y = torch.autograd.grad(\n        u_x, y,\n        torch.ones_like(u_x),\n        create_graph=True)[0]\n\n    # ∂u_y/∂x ~ ε_yx (strain component)\n    grad_u_y_x = torch.autograd.grad(\n        u_y, x,\n        torch.ones_like(u_y),\n        create_graph=True)[0]\n\n    # ∂u_y/∂y ~ ε_yy (strain component)\n    grad_u_y_y = torch.autograd.grad(\n        u_y, y,\n        torch.ones_like(u_y),\n        create_graph=True)[0]\n\n    \n    # ----------------------\n    # Constitutive relations\n    # Build stress tensor components using linear elasticity (plane strain)\n    # For isotropic linear elasticity:\n    #   1) σ_xx = (λ + 2μ) ∂u_x/∂x + λ ∂u_y/∂y\n    #   2) σ_yy = (λ + 2μ) ∂u_y/∂y + λ ∂u_x/∂x\n    #   3) σ_xy = μ (∂u_x/∂y + ∂u_y/∂x)\n    #   μ ~ shear modulus\n    # ----------------------\n    sigma_xx = (lambda_ + 2.0 * mu) * grad_u_x_x + lambda_ * grad_u_y_y   # 1)\n    sigma_yy = (lambda_ + 2.0 * mu) * grad_u_y_y + lambda_ * grad_u_x_x   # 2)\n    sigma_xy = mu * (grad_u_x_y + grad_u_y_x)                             # 3)\n\n    # ----------------------\n    # Derivatives of stresses: ∂σ_xx/∂x, ∂σ_xy/∂y, ∂σ_xy/∂x, ∂σ_yy/∂y\n    # ----------------------\n    sigma_xx_x = torch.autograd.grad(\n        sigma_xx, x,\n        torch.ones_like(sigma_xx),\n        create_graph=True)[0]\n    \n    sigma_xy_y = torch.autograd.grad(\n        sigma_xy, y,\n        torch.ones_like(sigma_xy),\n        create_graph=True)[0]\n    \n    sigma_xy_x = torch.autograd.grad(\n        sigma_xy, x,\n        torch.ones_like(sigma_xy),\n        create_graph=True)[0]\n    \n    sigma_yy_y = torch.autograd.grad(\n        sigma_yy, y,\n        torch.ones_like(sigma_yy),\n        create_graph=True)[0]\n\n    # Body force components at (x,y)\n    f_x = f_x_fun(x, y)\n    f_y = f_y_fun(x, y)\n\n    # ----------------------\n    # PDE residuals (equilibrium in x and y directions)\n    # ----------------------\n    r1 = sigma_xx_x + sigma_xy_y + f_x  # should be ~ 0\n    r2 = sigma_xy_x + sigma_yy_y + f_y  # should be ~ 0\n\n    # Mean squared residual over all interior points\n    return torch.mean(r1**2 + r2**2)\n\n\n\n# ============================================================\n# 5. Boundary condition loss L_BC\n#    Enforces displacement and traction conditions on:\n#      - y = 0\n#      - y = 1\n#      - x = 0\n#      - x = 1\n# ============================================================\n\ndef loss_bc():\n    \"\"\"\n    Compute boundary condition loss over all four sides of the domain.\n\n    Boundary conditions (schematic):\n\n      y = 0:\n        u_x = 0, u_y = 0      (clamped or fixed boundary)\n\n      y = 1:\n        u_x = 0, σ_yy = (λ + 2μ) Q sin(π x)   (prescribed normal traction in y)\n\n      x = 0:\n        u_y = 0, σ_xx = 0     (mix of displacement & traction conditions)\n\n      x = 1:\n        u_y = 0, σ_xx = 0\n\n    Each condition adds a mean-squared penalty term to loss_val.\n    \"\"\"\n    loss_val = 0.0\n    \n    # --------------------------------------------------------\n    # Boundary: y = 0  => u_x = 0, u_y = 0\n    # --------------------------------------------------------\n    # Extract x,y on this boundary and enable gradients for computing stresses if needed.\n    x0 = xy_bc_y0_torch[:, 0:1].clone().detach().requires_grad_(True)  # x ∈ [0,1]\n    y0 = xy_bc_y0_torch[:, 1:2].clone().detach().requires_grad_(True)  # y = 0\n\n    # Displacement at (x, 0)\n    u_bc_y0 = model(torch.cat((x0, y0), dim=1))  # shape (N_BC, 2)\n\n    # Enforce u_x(x,0) = 0 and u_y(x,0) = 0\n    loss_val += torch.mean(u_bc_y0[:, 0:1]**2)  # penalty for u_x\n    loss_val += torch.mean(u_bc_y0[:, 1:2]**2)  # penalty for u_y\n    \n    # --------------------------------------------------------\n    # Boundary: y = 1  => u_x = 0,  σ_yy = (λ+2μ) Q sin(π x)\n    # --------------------------------------------------------\n    x1 = xy_bc_y1_torch[:, 0:1].clone().detach().requires_grad_(True)  # x ∈ [0,1]\n    y1 = xy_bc_y1_torch[:, 1:2].clone().detach().requires_grad_(True)  # y = 1\n\n    u_bc_y1 = model(torch.cat((x1, y1), dim=1))  # displacement at (x,1)\n\n    # Enforce u_x(x,1) = 0\n    loss_val += torch.mean(u_bc_y1[:, 0:1]**2)\n\n    # For σ_yy, we need ∂u_y/∂y and ∂u_x/∂x at this boundary:\n    u_y_bc_y1 = u_bc_y1[:, 1:2]\n    u_y_bc_y1_y = torch.autograd.grad(\n        u_y_bc_y1, y1,\n        torch.ones_like(u_y_bc_y1),\n        create_graph=True\n    )[0]\n\n    u_x_bc_y1 = u_bc_y1[:, 0:1]\n    u_x_bc_y1_x = torch.autograd.grad(\n        u_x_bc_y1, x1,\n        torch.ones_like(u_x_bc_y1),\n        create_graph=True\n    )[0]\n\n    # σ_yy = (λ + 2μ) ∂u_y/∂y + λ ∂u_x/∂x\n    sigma_yy_y1 = (lambda_ + 2.0 * mu) * u_y_bc_y1_y + lambda_ * u_x_bc_y1_x\n\n    # Target traction σ_yy on y=1:\n    sigma_yy_target = (lambda_ + 2.0 * mu) * Q * torch.sin(np.pi * x1)\n\n    # Penalize deviation from prescribed traction\n    loss_val += torch.mean((sigma_yy_y1 - sigma_yy_target)**2)\n    \n    # --------------------------------------------------------\n    # Boundary: x = 0  => u_y = 0,  σ_xx = 0\n    # --------------------------------------------------------\n    x0_ = xy_bc_x0_torch[:, 0:1].clone().detach().requires_grad_(True)  # x = 0\n    y0_ = xy_bc_x0_torch[:, 1:2].clone().detach().requires_grad_(True)  # y ∈ [0,1]\n\n    u_bc_x0 = model(torch.cat((x0_, y0_), dim=1))  # displacement along x=0\n\n    # Enforce u_y(0,y) = 0\n    loss_val += torch.mean(u_bc_x0[:, 1:2]**2)\n\n    # For σ_xx, need ∂u_x/∂x and ∂u_y/∂y\n    u_x_bc_x0 = u_bc_x0[:, 0:1]\n    u_x_bc_x0_x = torch.autograd.grad(\n        u_x_bc_x0, x0_,\n        torch.ones_like(u_x_bc_x0),\n        create_graph=True\n    )[0]\n\n    u_y_bc_x0 = u_bc_x0[:, 1:2]\n    u_y_bc_x0_y = torch.autograd.grad(\n        u_y_bc_x0, y0_,\n        torch.ones_like(u_y_bc_x0),\n        create_graph=True\n    )[0]\n\n    # σ_xx = (λ + 2μ) ∂u_x/∂x + λ ∂u_y/∂y\n    sigma_xx_x0 = (lambda_ + 2.0 * mu) * u_x_bc_x0_x + lambda_ * u_y_bc_x0_y\n\n    # Enforce σ_xx(0,y) = 0\n    loss_val += torch.mean(sigma_xx_x0**2)\n    \n    # --------------------------------------------------------\n    # Boundary: x = 1  => u_y = 0,  σ_xx = 0\n    # --------------------------------------------------------\n    x1_ = xy_bc_x1_torch[:, 0:1].clone().detach().requires_grad_(True)  # x = 1\n    y1_ = xy_bc_x1_torch[:, 1:2].clone().detach().requires_grad_(True)  # y ∈ [0,1]\n\n    u_bc_x1 = model(torch.cat((x1_, y1_), dim=1))  # displacement along x=1\n\n    # Enforce u_y(1,y) = 0\n    loss_val += torch.mean(u_bc_x1[:, 1:2]**2)\n\n    # For σ_xx on x=1:\n    u_x_bc_x1 = u_bc_x1[:, 0:1]\n    u_x_bc_x1_x = torch.autograd.grad(\n        u_x_bc_x1, x1_,\n        torch.ones_like(u_x_bc_x1),\n        create_graph=True\n    )[0]\n\n    u_y_bc_x1 = u_bc_x1[:, 1:2]\n    u_y_bc_x1_y = torch.autograd.grad(\n        u_y_bc_x1, y1_,\n        torch.ones_like(u_y_bc_x1),\n        create_graph=True\n    )[0]\n\n    sigma_xx_x1 = (lambda_ + 2.0 * mu) * u_x_bc_x1_x + lambda_ * u_y_bc_x1_y\n\n    # Enforce σ_xx(1,y) = 0\n    loss_val += torch.mean(sigma_xx_x1**2)\n\n    return loss_val\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------------------\n# Optimizer setup\n#   - Adam optimizer updates all model parameters (weights + biases)\n#   - LR is a user-defined learning rate constant\n# ---------------------------------------------------------\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n# ---------------------------------------------------------\n# Main training loop\n#   - EPOCHS: total number of optimization steps\n#   - At each epoch:\n#       1) Reset gradients\n#       2) Compute PDE (interior) loss\n#       3) Compute boundary-condition loss\n#       4) Combine into total loss\n#       5) Backpropagate (compute gradients)\n#       6) Update parameters with optimizer\n# ---------------------------------------------------------\nfor ep in range(EPOCHS):\n    # Clear any gradients from the previous step\n    optimizer.zero_grad()\n\n    # PDE (interior) loss:\n    #   - xy_pde_torch contains interior collocation points (x, y)\n    #   - loss_pde measures violation of the PDE equilibrium at these points\n    loss_interior = loss_pde(xy_pde_torch)\n\n    # Boundary-condition loss:\n    #   - loss_bc enforces displacement and/or traction conditions\n    #     on the domain boundaries (e.g., x=0,1 or y=0,1)\n    loss_bound = loss_bc()\n\n    # Total loss = PDE residual loss + boundary loss\n    #   - As written, both contributions are equally weighted\n    #   - In practice, you can introduce weighting factors if needed\n    loss_total = loss_interior + loss_bound\n\n    # Backpropagation:\n    #   - Compute gradients of loss_total with respect to all trainable parameters\n    loss_total.backward()\n\n    # Optimizer step:\n    #   - Apply gradient-based updates to model parameters\n    optimizer.step()\n\n    # Print losses periodically for monitoring convergence\n    if ep % 500 == 0:\n        print(\n            f\"Epoch {ep:5d} | PDE Loss: {loss_interior.item():.4e} | \"\n            f\"BC Loss: {loss_bound.item():.4e} | Total: {loss_total.item():.4e}\"\n        )\n\nprint(\"Training finished.\")\n\n\n# ---------------------------------------------------------\n# Evaluation and visualization\n#   - Assume XX, YY define a regular 2D grid over the domain\n#   - Evaluate the trained PINN at each grid point to get u_x, u_y\n#   - Compare with exact solution fields u_x_ex, u_y_ex\n# ---------------------------------------------------------\n\n# Stack grid coordinates into a list of (x, y) points:\n#   - XX.ravel() and YY.ravel() flatten the 2D grids into 1D vectors\n#   - np.column_stack builds an array of shape (N_points, 2)\nXY_plot = np.column_stack((XX.ravel(), YY.ravel()))\n\n# Convert the coordinate array to a PyTorch tensor and move it to the chosen device\nXY_torch = torch.tensor(XY_plot, dtype=torch.float32).to(device)\n\n# Disable gradient tracking for evaluation to save memory and computation\nwith torch.no_grad():\n    # Evaluate the model at all grid points\n    # pred has shape (N_points, 2), where:\n    #   pred[:, 0] ≈ u_x(x, y)\n    #   pred[:, 1] ≈ u_y(x, y)\n    pred = model(XY_torch).cpu().numpy()\n\n# Reshape PINN predictions back to 2D fields matching XX, YY shapes:\n#   - u_x_pred and u_y_pred become (ny, nx) arrays\nu_x_pred = pred[:, 0].reshape((ny, nx))\nu_y_pred = pred[:, 1].reshape((ny, nx))\n\n# Compute exact (analytic) displacement fields on the same grid\n#   - exact_u_x(XX, YY) and exact_u_y(XX, YY) are user-defined functions\nu_x_ex = exact_u_x(XX, YY)\nu_y_ex = exact_u_y(XX, YY)\n\n\n\n# ---------------------------------------------------------\n# Plot 1: Compare u_x (PINN vs exact)\n# ---------------------------------------------------------\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left subplot: PINN-predicted u_x field\ncont1 = axs[0].contourf(XX, YY, u_x_pred, levels=30)\nfig.colorbar(cont1, ax=axs[0])\naxs[0].set_title(\"PINN predicted $u_x$\")\naxs[0].set_xlabel(\"x\")\naxs[0].set_ylabel(\"y\")\n\n# Right subplot: exact u_x field\ncont2 = axs[1].contourf(XX, YY, u_x_ex, levels=30)\nfig.colorbar(cont2, ax=axs[1])\naxs[1].set_title(\"Exact $u_x$\")\naxs[1].set_xlabel(\"x\")\naxs[1].set_ylabel(\"y\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n# ---------------------------------------------------------\n# Plot 2: Compare u_y (PINN vs exact)\n# ---------------------------------------------------------\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left subplot: PINN-predicted u_y field\ncont3 = axs[0].contourf(XX, YY, u_y_pred, levels=30)\nfig.colorbar(cont3, ax=axs[0])\naxs[0].set_title(\"PINN predicted $u_y$\")\naxs[0].set_xlabel(\"x\")\naxs[0].set_ylabel(\"y\")\n\n# Right subplot: exact u_y field\ncont4 = axs[1].contourf(XX, YY, u_y_ex, levels=30)\nfig.colorbar(cont4, ax=axs[1])\naxs[1].set_title(\"Exact $u_y$\")\naxs[1].set_xlabel(\"x\")\naxs[1].set_ylabel(\"y\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}