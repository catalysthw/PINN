{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade \"protobuf<5.0.0\"\n\nimport google.protobuf\nprint(\"protobuf version:\", google.protobuf.__version__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:46:46.355899Z","iopub.execute_input":"2025-11-26T14:46:46.356767Z","iopub.status.idle":"2025-11-26T14:46:52.571859Z","shell.execute_reply.started":"2025-11-26T14:46:46.356730Z","shell.execute_reply":"2025-11-26T14:46:52.570732Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf<5.0.0\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-4.25.8\nprotobuf version: 4.25.8\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:46:55.404353Z","iopub.execute_input":"2025-11-26T14:46:55.404688Z","iopub.status.idle":"2025-11-26T14:47:15.091930Z","shell.execute_reply.started":"2025-11-26T14:46:55.404656Z","shell.execute_reply":"2025-11-26T14:47:15.090645Z"}},"outputs":[{"name":"stderr","text":"2025-11-26 14:46:57.178212: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764168417.433629      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764168417.508553      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"### Defines the forward pass of NN. (Step 1: Define NN)\n\n# create_model(): defines and returns a d\ndef create_model():\n    model = {\n        'hidden_1': tf.keras.layers.Dense(50, activation='tanh'),\n        'hidden_2': tf.keras.layers.Dense(50, activation='tanh'),\n        'hidden_3': tf.keras.layers.Dense(50, activation='tanh'),\n        'output_layer': tf.keras.layers.Dense(1)\n    }\n    return model\n\ndef call_model(model, x):\n    x = model['hidden_1'](x)\n    x = model['hidden_2'](x)\n    x = model['hidden_3'](x)\n    x = model['output_layer'](x)\n    \n    return x\n\n### Define the PDE using tf.GradientTape (Step 2: Compute Derivatives)\n'''pde() evaluates the 2nd order PDE of a model's prediction. \n   'tf.GradientTape' is used to compute the 1st y' and 2nd y\" derivatives \n   of the model's output. 'tf.GradientTape' returns y\" + pi*sin(pi*x)\n'''\n\ndef pde(x, model):\n    \"\"\"\n    Compute the PDE residual for the governing equation:\n        y''(x) + pi^2 * sin(pi * x) = 0\n\n    Here, y(x) is approximated by a neural network y_hat(x) = N_theta(x).\n    This function returns:\n        r(x; theta) = d^2 y_hat / dx^2 + pi^2 * sin(pi * x)\n    which should be close to zero at the collocation points.\n    \"\"\"\n\n    # 'persistent=True' allows us to take higher-order derivatives (up to 2nd order here).\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(x)  # Tell the tape to treat x as a differentiable variable\n\n        # Forward pass through the neural network: y_hat(x) = N_theta(x)\n        y_pred = call_model(model,x)\n\n        # 1st order derivatives: dy_hat/dx\n        y_x = tape.gradient(y_pred,x)\n\n    # 2nd order derivatives: d^2 y_hat/dx^2    \n    y_xx = tape.gradient(y_x,x)\n\n    # Free the tape to release memory (no longer needed after second derivative)\n    del tape\n\n    # PDE residual: y\"(x) + pi^2 * sin(pi * x)\n    # For the true solution, this should be 0; during training we drive this residual toward 0.    \n    return y_xx + np.pi**2 * tf.sin(np.pi *x)\n\n\n# Define the loss function (Step 3~4: Define residual and loss function)\ndef loss(model, x, x_bc, y_bc):\n    res = pde(x, model)\n\n    # compute the mean squared error of the PDE residual at interior collocation points\n    loss_pde = tf.reduce_mean(tf.square(res))\n\n    # neural network prediction at boundary points x_bc\n    y_bc_pred = call_model(model, x_bc)\n\n    # compute the mean squared error of the boundary conditions\n    loss_bc = tf.reduce_mean(tf.square(y_bc - y_bc_pred))\n\n    # total loss = PDE loss + boundary-condition loss\n    return loss_pde + loss_bc\n\n\n# Training step (Step 5: Training)\ndef train_step(model, x, x_bc, y_bc, optimizer):\n    \"\"\"\n    Perform one training step:\n    1) Compute the total loss (PDE residual + boundary condition loss).\n    2) Compute gradients of the loss w.r.t. all trainable variables.\n    3) Apply the gradients using the given optimizer.\n    \"\"\"\n\n    # Record operations for automatic differentiation\n    with tf.GradientTape() as tape:\n        # Forward pass and loss computation\n        loss_val = loss(model, x, x_bc, y_bc)\n\n    # Collect all trainable variables from all layers in the model dictionary.\n    # This flattens:\n    #   [layer1.trainable_variables, layer2.trainable_variables, ...]\n    # into a single list: [W1, b1, W2, b2, ..., W_out, b_out] ~ kernel and bias (traini)\n    variables = [var\n                 for layer in model.values()\n                 for var in layer.trainable_variables]\n\n    # Compute gradients of the scalar loss w.r.t. each trainable variable.\n    # This is the backward pass: TensorFlow walks the computation graph\n    # from 'loss_val' back to each variable and applies the chain rule\n    # to obtain dL/dW, dL/db, etc.\n    # e.g.: grads = [dL/dW1, dL/db1, dL/dW2, dL/db2, ..., dL/dW_out, dL/db_out]\n    grads = tape.gradient(loss_val, variables)\n\n    # Apply gradients to update model parameters (kernel and biases)\n    # Apply the gradient updates: for each (grad, var) pair,\n    # the optimizer updates var <- var - lr * grad (or Adam-style update, etc.).\n    optimizer.apply_gradients(zip(grads, variables))\n\n    # Return the scalar loss value for logging/monitoring\n    return loss_val\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:47:40.465648Z","iopub.execute_input":"2025-11-26T14:47:40.466263Z","iopub.status.idle":"2025-11-26T14:47:40.479749Z","shell.execute_reply.started":"2025-11-26T14:47:40.466237Z","shell.execute_reply":"2025-11-26T14:47:40.478847Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#### Train/Test process\n\n# Generate interior training data (collocation points) for the PDE domain x ∈ [-1, 1]\nx_train = np.linspace(-1, 1, 100).reshape(-1, 1)   # shape: (100, 1)\nx_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n\n# Boundary data: x = -1 and x = 1, with boundary condition y(-1) = 0, y(1) = 0\nx_bc = np.array([[-1.0], [1.0]], dtype=np.float32)\ny_bc = np.array([[0.0], [0.0]], dtype=np.float32)\n\n# Convert boundary data to tensors (for use with TensorFlow ops)\nx_bc = tf.convert_to_tensor(x_bc, dtype=tf.float32)\ny_bc = tf.convert_to_tensor(y_bc, dtype=tf.float32)\n\n### Define the PINN model (a dictionary of layers: hidden_1, hidden_2, ..., output_layer)\nmodel = create_model()\n\n\n\n### Define the optimizer with a learning rate scheduler\n\n## Option 1: Adam with an exponential decay learning rate (currently active)\n\n# The learning rate starts at 1e-3 and decays by a factor of 0.9 every 1000 steps.\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-3,\n    decay_steps=1000,\n    decay_rate=0.9\n)\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n## Option 2: Adam with a constant learning rate\n## Use this if you want a fixed learning rate without decay.\n# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n\n## Option 3: SGD with momentum\n## Slower but more \"classical\" optimizer; sometimes good for fine control.\n# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=0.9)\n\n## Option 4: RMSprop\n## Often used for non-stationary problems, can be stable for many PINN tasks.\n# optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n\n## Option 5: Nadam (Adam + Nesterov momentum)\n## A variant of Adam that may converge a bit differently on some problems.\n# optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)\n\n\n\n### Training loop\n\n# Total number of training epochs (full passes over the training data)\nepochs = 2000  \n\nfor epoch in range(epochs):\n    # One training step:\n    # - Forward pass through the PINN\n    # - Compute PDE + boundary losses\n    # - Backward pass (compute gradients)\n    # - Update all trainable variables (weights and biases) using the optimizer\n    loss_value = train_step(model, x_train, x_bc, y_bc, optimizer)\n\n    # Print progress every 1000 epochs (you can change this frequency)\n    if epoch % 1000 == 0:\n        print(f\"Epoch {epoch}: Loss = {loss_value.numpy()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:47:45.742351Z","iopub.execute_input":"2025-11-26T14:47:45.742688Z","iopub.status.idle":"2025-11-26T14:49:53.629764Z","shell.execute_reply.started":"2025-11-26T14:47:45.742660Z","shell.execute_reply":"2025-11-26T14:49:53.628817Z"}},"outputs":[{"name":"stderr","text":"2025-11-26 14:47:45.749253: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\nEpoch 0: Loss = 47.71510314941406\nEpoch 1000: Loss = 0.0009805683512240648\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Predict the solution\nx_test = np.linspace(-1, 1, 1000).reshape(-1, 1)\nx_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\ny_pred = call_model(model, x_test).numpy()\n\n# Analytical solution\ny_true = np.sin(np.pi * x_test)\n\n\n# Plot the results\nplt.figure(figsize=(5, 3))\nplt.plot(x_test, y_true, 'b-', label='Analytical Solution')\nplt.plot(x_test, y_pred, 'r--', label='PINN Solution')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Comparison of Analytical Solution and PINN Solution')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport scipy.optimize  # for L-BFGS-B\n\n\n#### Two-stage parameter tuning\n### Stage 1: Adam pre-training\n\ndef train_with_adam(model, x_train, x_bc, y_bc,\n                    adam_epochs=2000, print_every=200):\n    \"\"\"\n    Stage 1: Use Adam to do rough optimization of the PINN parameters.\n    This gets the model into a reasonable basin of attraction before\n    running L-BFGS for fine-tuning.\n    \"\"\"\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=1e-3,\n        decay_steps=1000,\n        decay_rate=0.9\n    )\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n    for epoch in range(adam_epochs):\n        # One gradient-based update step using Adam\n        loss_value = train_step(model, x_train, x_bc, y_bc, optimizer)\n\n        # Optional: monitor training progress\n        if epoch % print_every == 0:\n            print(f\"[Adam] Epoch {epoch}: loss = {loss_value.numpy():.6e}\")\n\n    # Return: in-place model update\n\n\n\n### 2) Functions for Stage 2 (L-BFGS) \n## If a large scale data (over hundreds of thousands ~ millions) is sampled, \n## minibatch can be used for 2nd stage instead of L-BFGS\n\ndef train_with_lbfgs(model, x_train, x_bc, y_bc, maxiter=5000):\n    \n    # 1) Collect all trainable variables (W, b, etc.) from the model\n    variables = [var\n                 for layer in model.values()\n                 for var in layer.trainable_variables]\n\n    \n    # 2) Pre-compute shapes & sizes for each variable\n    #    (we will reuse these in both pack and unpack logic)\n    shapes = [v.shape for v in variables]\n    sizes = [int(np.prod(s)) for s in shapes]  # number of elements per variable\n\n    \n    # 3) Create initial parameter vector theta0 (flatten all variables)\n    flat_vars = [tf.reshape(v, [-1]) for v in variables]         # list of 1D tensors\n    theta0 = tf.concat(flat_vars, axis=0).numpy().astype(np.float64)  # 1D numpy array\n\n    \n    # 4) Define the objective function for SciPy: returns (loss, grad)\n    def objective(theta_np):\n        \"\"\"\n        theta_np: 1D numpy array containing all model parameters.\n        Returns:\n            loss_value (float)\n            grad_np (1D numpy array of gradients)\n        \"\"\"\n\n        # unpack step: update TF variables from theta_np ----\n        theta_tf = tf.convert_to_tensor(theta_np, dtype=tf.float32)\n\n        idx = 0\n        for v, size, shape in zip(variables, sizes, shapes):\n            # slice out the portion of theta corresponding to this variable\n            vals = theta_tf[idx:idx + size]\n            vals = tf.reshape(vals, shape)\n            v.assign(vals)             # in-place update of the TF variable\n            idx += size\n\n        \n        # compute loss and gradients with current parameters ----\n        with tf.GradientTape() as tape:\n            loss_val = loss(model, x_train, x_bc, y_bc)\n\n        grads = tape.gradient(loss_val, variables)\n\n        # ---- pack gradients into a single 1D vector ----\n        flat_grads = [tf.reshape(g, [-1]) for g in grads]          # list of 1D tensors\n        grad_flat = tf.concat(flat_grads, axis=0)                  # single 1D tensor\n\n        # SciPy expects float64 numpy results\n        loss_np = float(loss_val.numpy())\n        grad_np = grad_flat.numpy().astype(np.float64)\n\n        return loss_np, grad_np\n\n    \n    # 5) Call SciPy's L-BFGS-B optimizer\n    print(\"Starting L-BFGS-B (no helper functions)...\")\n\n    result = scipy.optimize.minimize(\n        fun=objective,\n        x0=theta0,\n        jac=True,           # objective returns (loss, grad)\n        method='L-BFGS-B',\n        options={\n            'maxiter': maxiter,\n            'disp': True,\n            'ftol': 1e-12,\n        }\n    )\n\n    print(\"L-BFGS-B finished.\")\n    print(\"  success :\", result.success)\n    print(\"  message :\", result.message)\n    print(\"  final loss:\", result.fun)\n\n    \n    # During optimization, 'objective' kept assigning the updated theta\n    # back into the TF variables, so the 'model' is already updated.\n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Full training pipeline\n\n### 1) Prepare data (same as before)\nx_train = np.linspace(-1, 1, 100).reshape(-1, 1).astype(np.float32)\nx_train = tf.convert_to_tensor(x_train)\n\nx_bc = np.array([[-1.0], [1.0]], dtype=np.float32)\ny_bc = np.array([[0.0], [0.0]], dtype=np.float32)\nx_bc = tf.convert_to_tensor(x_bc)\ny_bc = tf.convert_to_tensor(y_bc)\n\n### 2) Create PINN model\nmodel = create_model()\n\n### 3) Stage 1: Adam pre-training\ntrain_with_adam(model, x_train, x_bc, y_bc,\n                adam_epochs=2000, print_every=200)\n\n### 4) Stage 2: L-BFGS-B fine-tuning\nlbfgs_result = train_with_lbfgs(model, x_train, x_bc, y_bc,\n                                maxiter=5000)\n\n### 5) Evaluate and visualize\n\n## Fine grid for plotting\nx_test = np.linspace(-1.0, 1.0, 200).reshape(-1, 1).astype(np.float32)\nx_test_tf = tf.convert_to_tensor(x_test)\n\n## PINN prediction after two-stage training\ny_pred = call_model(model, x_test_tf).numpy()\n\n## Analytical solution for this PDE:\n# y''(x) + pi^2 sin(pi x) = 0, y(-1)=0, y(1)=0  →  y(x) = sin(pi x)\ny_true = np.sin(np.pi * x_test)\n\n## Mean squared error on the test grid\nmse_test = np.mean((y_true - y_pred) ** 2)\nprint(f\"\\nTest MSE on grid: {mse_test:.6e}\")\n\n## Figure 1: true solution vs PINN prediction\nplt.figure(figsize=(6, 4))\nplt.plot(x_test, y_true, label=\"Analytical solution: sin(pi x)\")\nplt.plot(x_test, y_pred, \"--\", label=\"PINN prediction\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"PDE solution: true vs PINN (Adam + L-BFGS)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n## Figure 2: prediction error\nplt.figure(figsize=(6, 4))\nplt.plot(x_test, y_pred - y_true)\nplt.xlabel(\"x\")\nplt.ylabel(\"Error\")\nplt.title(\"PINN error: y_pred(x) - y_true(x)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### Two-stage parameter tuning\n### Stage 1: Adam pre-training\n\n## Stage 1 function: Adam pre-training (full-batch)\ndef train_with_adam_fullbatch(model, x_train, x_bc, y_bc,\n                              adam_epochs=2000, print_every=200):\n    \"\"\"\n    Stage 1:\n    Use full-batch Adam to get the parameters into a good basin.\n    \"\"\"\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=1e-3,\n        decay_steps=1000,\n        decay_rate=0.9,\n    )\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n    for epoch in range(adam_epochs):\n        loss_value = train_step(model, x_train, x_bc, y_bc, optimizer)\n\n        if epoch % print_every == 0:\n            print(f\"[Stage 1 - Adam full] Epoch {epoch}: loss = {loss_value.numpy():.6e}\")\n\n\n## Stage 2 function: mini-batch fine-tuning\ndef train_with_minibatch(model, x_train, x_bc, y_bc,\n                         epochs=20, batch_size=256, lr=1e-4):\n    \"\"\"\n    Stage 2:\n    Use mini-batch Adam with a smaller learning rate\n    for fine-tuning the parameters.\n\n    This is especially useful when the number of interior points is large.\n    \"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices(x_train)\n    dataset = dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n    for epoch in range(epochs):\n        for x_batch in dataset:\n            loss_value = train_step_batch(model, x_batch, x_bc, y_bc, optimizer)\n        print(f\"[Stage 2 - Mini-batch] Epoch {epoch}: loss = {loss_value.numpy():.6e}\")\n\n\n\n## Interior collocation points x ∈ [-1, 1]\nx_train = np.linspace(-1.0, 1.0, 100).reshape(-1, 1).astype(np.float32)\nx_train = tf.convert_to_tensor(x_train)\n\n## Boundary points and values: y(-1) = 0, y(1) = 0\nx_bc = np.array([[-1.0], [1.0]], dtype=np.float32)\ny_bc = np.array([[0.0], [0.0]], dtype=np.float32)\nx_bc = tf.convert_to_tensor(x_bc)\ny_bc = tf.convert_to_tensor(y_bc)\n\n\n### Train the PINN\n\n# Create model\nmodel = create_model()\n\n# Stage 1: full-batch Adam\ntrain_with_adam_fullbatch(model, x_train, x_bc, y_bc,\n                          adam_epochs=2000, print_every=200)\n\n# Stage 2: mini-batch fine-tuning\n# (here still using the same x_train, but treated as if it's large;\n# in a real large-scale case, x_train would have many more points)\ntrain_with_minibatch(model, x_train, x_bc, y_bc,\n                     epochs=50, batch_size=32, lr=1e-4)\n\n\n\n# 6. Evaluate and visualize results\n\n# Create a fine grid for visualization\nx_test = np.linspace(-1.0, 1.0, 200).reshape(-1, 1).astype(np.float32)\nx_test_tf = tf.convert_to_tensor(x_test)\n\n# PINN prediction\ny_pred = call_model(model, x_test_tf).numpy()\n\n# Analytical solution: y(x) = sin(pi * x)\ny_true = np.sin(np.pi * x_test)\n\n# Mean squared error on the test grid\nmse_test = np.mean((y_true - y_pred) ** 2)\nprint(f\"\\nTest MSE on grid: {mse_test:.6e}\")\n\n# Plot: true solution vs PINN prediction\nplt.figure(figsize=(6, 4))\nplt.plot(x_test, y_true, label=\"Analytical solution: sin(pi x)\")\nplt.plot(x_test, y_pred, \"--\", label=\"PINN prediction\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"PDE solution: true vs PINN (two-stage training)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Optional: plot the error\nplt.figure(figsize=(6, 4))\nplt.plot(x_test, y_pred - y_true)\nplt.xlabel(\"x\")\nplt.ylabel(\"Prediction error\")\nplt.title(\"PINN error: y_pred(x) - y_true(x)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}