{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize alpha values\ndef normalise_alpha(alpha):\n    \"\"\"\n    Map the physical parameter alpha ∈ [1,10] to a normalized value in [0,1].\n\n    Reason:\n      - Neural networks usually train better if all inputs are on a similar scale\n        (e.g., ~[0,1] or [-1,1]).\n      - Here, alpha is originally in [1,10], so we linearly rescale it.\n\n    Formula:\n      alpha_norm = (alpha - 1) / 9\n      so that alpha = 1 -> 0, alpha = 10 -> 1.\n    \"\"\"\n    return (alpha - 1.) / 9.\n\n# Denormalize alpha values\ndef denormalise_alpha(alpha_tilde):\n    \"\"\"\n    Inverse mapping: convert normalized alpha_tilde ∈ [0,1] back to physical alpha ∈ [1,10].\n\n    Formula:\n      alpha = alpha_tilde * 9 + 1\n      so that alpha_tilde = 0 -> 1, alpha_tilde = 1 -> 10.\n    \"\"\"\n    return alpha_tilde * 9. + 1.\n\n\n# Collocation points --------------------------------------------------------\n# Number of interior (x, alpha) points where we enforce the PDE residual.\nn_collocation = 10_000\n\n# Sample x uniformly in the spatial domain [0, 1].\n# Shape: (n_collocation, 1)\nx_colloc = torch.rand(n_collocation, 1, device=device)\n\n# Sample alpha uniformly in the physical range [1, 10].\n# Shape: (n_collocation, 1)\nalpha_colloc = 1 + 9 * torch.rand(n_collocation, 1, device=device)  # physical alpha\n\n# Normalized alpha used as NN input (better numerical behavior).\nalpha_colloc_norm = normalise_alpha(alpha_colloc)\n\n\n# Boundary points -----------------------------------------------------------\n# Number of boundary points used to enforce u(0, alpha) = 0 and u(1, alpha) = 0.\nn_boundary = 1000\n\n# Sample alpha from the same distribution [1, 10] for boundary points\n# so that boundary conditions are enforced across the full parameter range.\nalpha_bc = 1 + 9 * torch.rand(n_boundary, 1, device=device)\nalpha_bc_norm = normalise_alpha(alpha_bc)\n\n# Left boundary: x = 0, for all sampled alpha_bc.\nx_bc_left  = torch.zeros_like(alpha_bc, device=device)  # shape (n_boundary, 1)\n\n# Right boundary: x = 1, for all sampled alpha_bc.\nx_bc_right = torch.ones_like(alpha_bc, device=device)   # shape (n_boundary, 1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Parametric PINN model -----------------------------------------------------\nclass ParametricPINN(nn.Module):\n    def __init__(self, layers=(2, 64, 64, 64, 64, 1)):\n        \"\"\"\n        layers: tuple specifying network architecture:\n          (input_dim, hidden1, hidden2, ..., hiddenK, output_dim)\n\n        Here:\n          (2, 64, 64, 64, 64, 1) means:\n            - input_dim = 2  (x, alpha_norm)\n            - 4 hidden layers with 64 neurons each\n            - output_dim = 1  (scalar u(x; alpha))\n        \"\"\"\n        super().__init__()\n\n        net_layers = []\n        # Build all hidden layers with Tanh activation\n        # layers[:-2] = all except last two: (2, 64, 64, 64)\n        # layers[1:-1] = from second to second-last: (64, 64, 64, 64)\n        # zip(...) gives pairs: (2->64), (64->64), (64->64), (64->64)\n        for in_dim, out_dim in zip(layers[:-2], layers[1:-1]):\n            net_layers.append(nn.Linear(in_dim, out_dim))\n            net_layers.append(nn.Tanh())\n\n        # Final layer: maps last hidden size -> output_dim (no activation)\n        net_layers.append(nn.Linear(layers[-2], layers[-1]))\n\n        # Wrap into a Sequential module for easy forward pass.\n        self.network = nn.Sequential(*net_layers)\n\n    def forward(self, x, alpha_norm):\n        \"\"\"\n        Forward pass of the parametric PINN.\n\n        Inputs:\n          x          : tensor of shape (N, 1), spatial coordinates in [0,1]\n          alpha_norm : tensor of shape (N, 1), normalized alpha in [0,1]\n\n        We concatenate x and alpha_norm along the feature dimension so that\n        each input sample is [x_i, alpha_norm_i].\n\n        Output:\n          u(x, alpha_norm): shape (N, 1), approximate solution of PDE.\n        \"\"\"\n        # Concatenate spatial and parameter inputs -> shape (N, 2)\n        inputs = torch.cat([x, alpha_norm], dim=1)\n        return self.network(inputs)\n\n\n# Instantiate the PINN and move it to the desired device (CPU/GPU).\npinn = ParametricPINN().to(device)\n\n\n# PDE residual computation --------------------------------------------------\ndef pde_residual(model, x, alpha, alpha_norm):\n    \"\"\"\n    Compute the residual of the PDE:\n\n        u_xx(x; alpha) + alpha = 0\n\n    We return:\n        r(x, alpha) = u_xx(x; alpha) + alpha\n\n    so that the PINN will try to make r ≈ 0 at collocation points.\n\n    Inputs:\n      model      : ParametricPINN instance\n      x          : (N,1) tensor of spatial coordinates\n      alpha      : (N,1) tensor of *physical* alpha values\n      alpha_norm : (N,1) tensor of *normalized* alpha values (used as NN input)\n\n    Note:\n      - We do NOT differentiate w.r.t. alpha here, only w.r.t. x.\n      - alpha is treated as constant in the PDE (parameter, not variable).\n    \"\"\"\n    # Enable gradients w.r.t. x so autograd can compute u_x and u_xx.\n    x.requires_grad_(True)\n    # Alpha_norm is not used for derivatives, so we keep requires_grad = False.\n    alpha_norm.requires_grad_(False)\n\n    # Forward pass: u(x; alpha_norm)\n    u = model(x, alpha_norm)  # shape (N,1)\n\n    # First derivative du/dx.\n    #   grad_outputs=torch.ones_like(u) selects elementwise derivatives.\n    u_x = autograd.grad(\n        u, x,\n        grad_outputs=torch.ones_like(u),\n        create_graph=True)[0]\n\n    # Second derivative d^2u/dx^2 by differentiating u_x w.r.t x again.\n    u_xx = autograd.grad(\n        u_x, x,\n        grad_outputs=torch.ones_like(u_x),\n        create_graph=True)[0]\n\n    # PDE residual: u_xx + alpha should be zero ideally.\n    # -d^2u/dx^2 = alpha ~> d^2u/dx^2 + alpha = 0\n    return u_xx + alpha  # residual ~ 0 at collocation points\n\n\n\n# Optimizer and training ----------------------------------------------------\noptimizer = torch.optim.Adam(pinn.parameters(), lr=1e-3)\n\n# Weight for pde term in total loss.\nW_pde = 1.0\n\n# Weight for boundary condition term in total loss.\nW_bc = 1.0\n\n\ndef train(num_iterations=10_000, print_every=1000):\n    \"\"\"\n    Train the parametric PINN using a simple gradient-descent loop.\n\n    Loss = PDE residual loss + W_bc * boundary condition loss\n\n    PDE residual loss:\n      - Mean-squared error of r(x, alpha) = u_xx + alpha at collocation points.\n\n    Boundary condition loss:\n      - Enforce u(0, alpha) = 0 and u(1, alpha) = 0 for randomly sampled alphas.\n    \"\"\"\n    for it in range(1, num_iterations + 1):\n        optimizer.zero_grad()\n\n        # ---------------------------\n        # PDE residual loss (interior)\n        # ---------------------------\n        res = pde_residual(pinn, x_colloc, alpha_colloc, alpha_colloc_norm)\n        loss_pde = torch.mean(res**2)\n\n        # ---------------------------\n        # Boundary condition loss\n        #   BC: u(0; alpha) = 0, u(1; alpha) = 0\n        # ---------------------------\n        u_left  = pinn(x_bc_left,  alpha_bc_norm)  # u(0; alpha)\n        u_right = pinn(x_bc_right, alpha_bc_norm)  # u(1; alpha)\n\n        # Penalize deviations from zero at both boundaries.\n        loss_bc = torch.mean(u_left**2) + torch.mean(u_right**2)\n\n        # Total loss combines PDE and BC terms.\n        loss = W_pde * loss_pde + W_bc * loss_bc\n\n        # Backpropagation: compute d(loss)/d(parameters)\n        loss.backward()\n\n        # Update network parameters with Adam optimizer\n        optimizer.step()\n\n        # Console logging\n        if it % print_every == 0 or it == 1:\n            print(\n                f\"Iter {it:6d} | Loss: {loss.item():.4e} \"\n                f\"| PDE: {loss_pde.item():.2e} | BC: {loss_bc.item():.2e}\"\n            )\n\n\n# Uncomment the next line to train (takes ~1–2 min on CPU)\ntrain()\n\n\n# True analytical solution --------------------------------------------------\ndef true_solution(x, alpha):\n    \"\"\"\n    Closed-form solution of the boundary value problem:\n\n        u_xx + alpha = 0,   x ∈ [0,1]\n        u(0) = 0, u(1) = 0\n\n    Solve ODE:\n      u''(x) = -alpha\n      Integrate twice:\n         u(x) = -alpha * x^2 / 2 + C1 * x + C2\n\n      Apply boundary conditions:\n         u(0) = 0 → C2 = 0\n         u(1) = 0 → -alpha/2 + C1 = 0 → C1 = alpha/2\n\n      So:\n         u(x) = (alpha/2) * x * (1 - x)\n    \"\"\"\n    return (alpha / 2.0) * x * (1 - x)\n\n\n# Visualization -------------------------------------------------------------\ndef plot_predictions(model, alphas=[1, 3, 5, 7, 10], num_points=200):\n    \"\"\"\n    Plot PINN predictions vs. analytical solution for several alpha values.\n\n    Inputs:\n      model     : trained ParametricPINN\n      alphas    : list of physical alpha values to evaluate\n      num_points: number of spatial points x in [0,1] for plotting\n\n    For each alpha in 'alphas':\n      - Build x grid in [0,1]\n      - Compute normalized alpha input\n      - Get PINN prediction u_pred(x; alpha)\n      - Compute true solution u_true(x; alpha)\n      - Plot both curves on the same axes.\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n\n    # Spatial grid x ∈ [0,1]\n    x = torch.linspace(0, 1, num_points, device=device).unsqueeze(1)  # shape (num_points,1)\n\n    for a in alphas:\n        # Constant alpha field of shape (num_points,1)\n        a_torch = torch.full_like(x, float(a))\n        # Normalize to [0,1] for NN input\n        a_norm  = normalise_alpha(a_torch)\n\n        # PINN prediction for this alpha\n        with torch.no_grad():\n            u_pred = model(x, a_norm).cpu().numpy().flatten()\n\n        # Analytical solution (true) evaluated on same x\n        u_true = true_solution(x.cpu().numpy(), a).flatten()\n\n        # Plot PINN prediction\n        plt.plot(x.cpu().numpy(), u_pred, label=f\"PINN α={a}\")\n        # Plot true solution as dashed line\n        plt.plot(x.cpu().numpy(), u_true, linestyle='--')\n\n    plt.title(\"Parametric PINN Prediction – Constant RHS\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"u(x; α)\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\n# Uncomment after training to visualize results.\nplot_predictions(pinn)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}