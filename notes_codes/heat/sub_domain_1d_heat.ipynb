{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "sub_domain_1d_heat",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "FoJKx_MO5Vwo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import LBFGS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-11T23:10:25.368491Z",
          "iopub.execute_input": "2025-12-11T23:10:25.368794Z",
          "iopub.status.idle": "2025-12-11T23:10:30.441003Z",
          "shell.execute_reply.started": "2025-12-11T23:10:25.368771Z",
          "shell.execute_reply": "2025-12-11T23:10:30.439908Z"
        },
        "id": "Blfit_YR5Vwp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.optim import Adam, LBFGS # Optimizer imports\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. GLOBAL CONFIGURATION AND HYPERPARAMETERS\n",
        "# ==============================================================================\n",
        "\n",
        "# Device setting\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Physical/Domain Parameters\n",
        "x_if = 0.5                              # Interface location\n",
        "L = 1.0                                 # Total domain length (0 to 1)\n",
        "n = 1                                   # Parameter for initial condition (sin(n*pi*x/L))\n",
        "alpha = 0.1                             # Thermal diffusivity (Example value)\n",
        "\n",
        "# Exact solution\n",
        "def u_exact(x, t):\n",
        "    return np.exp(-n**2*np.pi**2*Î±*t/L**2) * np.sin(n*np.pi*x/L)\n",
        "\n",
        "# Sampling Points\n",
        "N_int = 8000                            # Collocation points (PDE)\n",
        "N_b = 400                               # Boundary points (BC)\n",
        "N_0 = 400                               # Initial condition points (IC)\n",
        "N_if = 800                              # Interface points (IF)\n",
        "\n",
        "# Training Parameters\n",
        "epochs_adam = 8000\n",
        "print_every = 500\n",
        "lr_adam = 1e-3                          # Adam Optimizer learning rate\n",
        "\n",
        "# Loss Weights (Adjusted weights for better convergence in boundary-heavy problems)\n",
        "W_PDE = 1\n",
        "W_BC  = 5\n",
        "W_IC  = 5\n",
        "W_IF  = 5\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. NETWORK DEFINITION (PINN Class)\n",
        "# ==============================================================================\n",
        "\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self, layers=(2, 128, 128, 128, 1)):\n",
        "        super(PINN, self).__init__()\n",
        "\n",
        "        # --- 1. Network Architecture Construction ---\n",
        "        seq = []\n",
        "        for i in range(len(layers) - 2):\n",
        "            seq.append(nn.Linear(layers[i], layers[i+1]))\n",
        "            seq.append(nn.Tanh())\n",
        "\n",
        "        # Final output layer\n",
        "        seq.append(nn.Linear(layers[-2], layers[-1]))\n",
        "\n",
        "        # Combine and move to device\n",
        "        self.net = nn.Sequential(*seq).to(device)\n",
        "\n",
        "        # --- 2. Weight Initialization ---\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for p in self.net.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. SAMPLING AND RESIDUAL FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def sampler(N, x_min, x_max):\n",
        "    \"\"\"Sample x uniformly in [x_min, x_max) and t uniformly in [0, 1)\"\"\"\n",
        "    x = (x_max - x_min) * torch.rand(N, 1) + x_min\n",
        "    t = torch.rand(N, 1)\n",
        "    return x.to(device), t.to(device)\n",
        "\n",
        "def time_sampler(N):\n",
        "    \"\"\"Sample time t uniformly in [0, 1)\"\"\"\n",
        "    return torch.rand(N, 1).to(device)\n",
        "\n",
        "def initial_sampler(N, x_min, x_max):\n",
        "    \"\"\"Sample x and set time t=0\"\"\"\n",
        "    x = (x_max - x_min) * torch.rand(N, 1) + x_min\n",
        "    t = torch.zeros_like(x)\n",
        "    return x.to(device), t.to(device)\n",
        "\n",
        "def pde_residual(net, x, t):\n",
        "    \"\"\"Calculates the residual of the 1D Heat Equation: du/dt - alpha * d2u/dx2\"\"\"\n",
        "    x.requires_grad_(True)\n",
        "    t.requires_grad_(True)\n",
        "\n",
        "    # Forward pass\n",
        "    u = net(torch.cat((x, t), 1))\n",
        "\n",
        "    # Calculate derivatives using torch.autograd.grad\n",
        "    u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
        "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
        "    u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x), create_graph=True)[0]\n",
        "\n",
        "    # PDE Residual\n",
        "    residual = u_t - alpha * u_xx\n",
        "\n",
        "    return residual\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. LOSS FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def loss_pde(netL, netR):\n",
        "    \"\"\"Loss from the PDE residual.\"\"\"\n",
        "    xL, tL = sampler(N_int, 0, x_if)\n",
        "    res_L = pde_residual(netL, xL, tL).pow(2).mean()\n",
        "    xR, tR = sampler(N_int, x_if, L)\n",
        "    res_R = pde_residual(netR, xR, tR).pow(2).mean()\n",
        "    return res_L + res_R\n",
        "\n",
        "def loss_bc(netL, netR):\n",
        "    \"\"\"Loss from Dirichlet Boundary Conditions (u=0 at x=0, x=L).\"\"\"\n",
        "    t_b = time_sampler(N_b)\n",
        "    # x=0 boundary (Left)\n",
        "    bc_L = netL(torch.cat((torch.zeros_like(t_b), t_b), 1)).pow(2).mean()\n",
        "    # x=L boundary (Right)\n",
        "    bc_R = netR(torch.cat((torch.full_like(t_b, L), t_b), 1)).pow(2).mean()\n",
        "    return bc_L + bc_R\n",
        "\n",
        "def loss_ic(netL, netR):\n",
        "    \"\"\"Loss from Initial Condition (t=0). u(x, 0) = sin(n*pi*x/L)\"\"\"\n",
        "    x0L, t0L = initial_sampler(N_0, 0, x_if)\n",
        "    ic_L = (netL(torch.cat((x0L, t0L), 1)) - torch.sin(n * np.pi * x0L / L)).pow(2).mean()\n",
        "    x0R, t0R = initial_sampler(N_0, x_if, L)\n",
        "    ic_R = (netR(torch.cat((x0R, t0R), 1)) - torch.sin(n * np.pi * x0R / L)).pow(2).mean()\n",
        "    return ic_L + ic_R\n",
        "\n",
        "def loss_interface(netL, netR):\n",
        "    \"\"\"Loss from Interface Continuity (uL=uR and du/dxL=du/dxR) at x=x_if.\"\"\"\n",
        "\n",
        "    # Sample time points (t_if) uniformly across the time domain [0, T]\n",
        "    t_if = time_sampler(N_if)\n",
        "\n",
        "    # Create the tensor representing the interface x-coordinate (x_if) for all sampled time points.\n",
        "    x_if_tensor = torch.full_like(t_if, x_if).to(device)\n",
        "\n",
        "    # Value continuity (uL = uR)\n",
        "    x_ifL_val = x_if_tensor.clone()\n",
        "    x_ifL_val.requires_grad_(False)\n",
        "    x_ifR_val = x_if_tensor.clone()\n",
        "    x_ifR_val.requires_grad_(False)\n",
        "\n",
        "    # Perform the forward pass for the Left/Right networks (netL/netR) using the combined [x, t] inputs.\n",
        "    uL = netL(torch.cat((x_ifL_val, t_if), 1))\n",
        "    uR = netR(torch.cat((x_ifR_val, t_if), 1))\n",
        "\n",
        "    # Mean Squared Error (MSE) loss for Value Continuity: (uL - uR)^2.\n",
        "    # Minimizing this forces uL to be equal to uR at the interface points.\n",
        "    cont_val = (uL - uR).pow(2).mean()\n",
        "\n",
        "    # Flux continuity (du/dxL = du/dxR)\n",
        "    # Clone the base x_if_tensor for the Left network's flux evaluation.\n",
        "    x_ifL_flux = x_if_tensor.clone()\n",
        "    # (du/dx) with respect to this tensor using Autograd.\n",
        "    x_ifL_flux.requires_grad_(True)\n",
        "\n",
        "    x_ifR_flux = x_if_tensor.clone()\n",
        "    x_ifR_flux.requires_grad_(True)\n",
        "\n",
        "    # Forward pass to get u values (uL_flux / uR_flux) needed for du/dx calculation.\n",
        "    uL_flux = netL(torch.cat((x_ifL_flux, t_if), 1))\n",
        "    uR_flux = netR(torch.cat((x_ifR_flux, t_if), 1))\n",
        "\n",
        "    # Compute the derivative du/dx for the Left/Right networks (uL_dx / uR_dx).\n",
        "    # - inputs = x_ifL_flux: Specifies the variable (x) to differentiate with respect to.\n",
        "    # - grad_outputs=torch.ones_like(uL_flux): Required because uL_flux is not a scalar.\n",
        "    # - create_graph=True: Required to calculate second-order derivatives (used in the\n",
        "    #   overall loss backward pass and optimization).\n",
        "    uL_dx = torch.autograd.grad(uL_flux, x_ifL_flux, torch.ones_like(uL_flux), create_graph=True)[0]\n",
        "    uR_dx = torch.autograd.grad(uR_flux, x_ifR_flux, torch.ones_like(uR_flux), create_graph=True)[0]\n",
        "\n",
        "    cont_flux = (uL_dx - uR_dx).pow(2).mean()\n",
        "\n",
        "    return cont_val + cont_flux\n",
        "\n",
        "def total_loss_fn(netL, netR):\n",
        "    \"\"\"Aggregates all individual loss components with their respective weights.\"\"\"\n",
        "\n",
        "    L_PDE = loss_pde(netL, netR)\n",
        "    L_BC = loss_bc(netL, netR)\n",
        "    L_IC = loss_ic(netL, netR)\n",
        "    L_IF = loss_interface(netL, netR)\n",
        "\n",
        "    # Total weighted loss\n",
        "    total_loss = (W_PDE * L_PDE) + (W_BC * L_BC) + \\\n",
        "                 (W_IC * L_IC) + (W_IF * L_IF)\n",
        "\n",
        "    return total_loss, L_PDE, L_BC, L_IC, L_IF\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MODEL INSTANTIATION AND TRAINING SETUP\n",
        "# ==============================================================================\n",
        "\n",
        "netL = PINN().to(device) # Network for the Left domain\n",
        "netR = PINN().to(device) # Network for the Right domain\n",
        "\n",
        "# Combine parameters from both networks for the optimizer\n",
        "params = list(netL.parameters()) + list(netR.parameters())\n",
        "\n",
        "# Adam Optimizer initialization\n",
        "opt_adam = Adam(params, lr=lr_adam)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. ADAM OPTIMIZATION (PRE-TRAINING)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"--- Starting Adam Optimization (Pre-training) ---\")\n",
        "for ep in range(1, epochs_adam + 1):\n",
        "    opt_adam.zero_grad()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Forward Pass and Total Loss Calculation\n",
        "    # Executes the forward pass of both netL and netR on sampled collocation points.\n",
        "    # The total_loss_fn encapsulates all physical constraints:\n",
        "    # - L_PDE: Physics Loss (Residual of the Heat Equation)\n",
        "    # - L_BC: Boundary Condition Loss (e.g., u=0 at x=0, x=L)\n",
        "    # - L_IC: Initial Condition Loss (u(x, 0) = Initial State)\n",
        "    # - L_IF: Interface Continuity Loss (uL=uR, du/dxL=du/dxR at the domain split point)\n",
        "    # l_total is the weighted sum of these individual losses.\n",
        "    l_total, l_pde, l_bc, l_ic, l_if = total_loss_fn(netL, netR)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Backpropagation (Gradient Calculation)\n",
        "    # Initiates the backward pass using PyTorch's Autograd engine.\n",
        "    # This computes the partial derivative of the total loss (l_total) with respect\n",
        "    # to every trainable parameter (weights and biases) in netL and netR.\n",
        "    # The calculated gradients (dL/dW) are stored in the '.grad' attribute of each parameter Tensor.\n",
        "    l_total.backward()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # The Adam optimizer uses the calculated gradients (from l_total.backward())\n",
        "    # to update the model parameters according to the Adam algorithm (Adaptive Moment Estimation).\n",
        "    # This is where the weights and biases are actually adjusted to minimize the total loss.\n",
        "    opt_adam.step()\n",
        "\n",
        "    # Logging and Printing\n",
        "    if ep % print_every == 0:\n",
        "        print(f\"Epoch {ep:5d} | Total Loss={l_total.item():.3e} | PDE={l_pde.item():.3e} | BC={l_bc.item():.3e} | IC={l_ic.item():.3e} | IF={l_if.item():.3e}\")\n",
        "\n",
        "print(\"Adam Optimization finished.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. L-BFGS OPTIMIZATION (REFINEMENT)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n--- Starting LBFGS Refinement ---\")\n",
        "\n",
        "# LBFGS Optimizer initialization\n",
        "lbfgs = LBFGS(params,\n",
        "              lr=1.0,\n",
        "              max_iter=500,\n",
        "              max_eval=500,\n",
        "              tolerance_grad=1e-8,\n",
        "              tolerance_change=1e-9,\n",
        "              history_size=200,\n",
        "              line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "# Counter for LBFGS printing\n",
        "lbfgs_step_counter = 0\n",
        "\n",
        "def lbfgs_closure_with_print():\n",
        "    \"\"\"LBFGS closure function with printing logic.\"\"\"\n",
        "    global lbfgs_step_counter\n",
        "    lbfgs_step_counter += 1\n",
        "\n",
        "    lbfgs.zero_grad()\n",
        "\n",
        "    l_total, l_pde, l_bc, l_ic, l_if = total_loss_fn(netL, netR)\n",
        "\n",
        "    l_total.backward()\n",
        "\n",
        "    if lbfgs_step_counter % 50 == 0:\n",
        "        print(f\"LBFGS Step {lbfgs_step_counter:3d} | Total Loss={l_total.item():.3e} | PDE={l_pde.item():.3e}\")\n",
        "\n",
        "    return l_total\n",
        "\n",
        "# Run LBFGS optimization\n",
        "lbfgs.step(lbfgs_closure_with_print)\n",
        "\n",
        "# Final loss calculation after optimization\n",
        "final_loss, _, _, _, _ = total_loss_fn(netL, netR)\n",
        "print(f\"LBFGS Optimization finished. Final loss = {final_loss.item():.3e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-11T23:24:09.741503Z",
          "iopub.execute_input": "2025-12-11T23:24:09.742447Z",
          "iopub.status.idle": "2025-12-12T00:09:44.752699Z",
          "shell.execute_reply.started": "2025-12-11T23:24:09.742417Z",
          "shell.execute_reply": "2025-12-12T00:09:44.75134Z"
        },
        "id": "U6L6qtwM5Vwq",
        "outputId": "a0610b9c-3584-4ae1-dc74-0c4dfefeb2c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Starting Adam Optimization (Pre-training) ---\nEpoch   500 | Total Loss=1.867e-01 | PDE=1.465e-02 | BC=3.939e-03 | IC=3.094e-03 | IF=2.739e-02\nEpoch  1000 | Total Loss=1.236e-02 | PDE=2.364e-03 | BC=2.074e-04 | IC=8.231e-05 | IF=1.710e-03\nEpoch  1500 | Total Loss=1.785e-03 | PDE=1.473e-03 | BC=9.233e-06 | IC=2.215e-05 | IF=3.100e-05\nEpoch  2000 | Total Loss=2.998e-03 | PDE=1.119e-03 | BC=4.953e-05 | IC=6.780e-05 | IF=2.586e-04\nEpoch  2500 | Total Loss=9.281e-02 | PDE=1.116e-03 | BC=3.561e-03 | IC=3.126e-03 | IF=1.165e-02\nEpoch  3000 | Total Loss=9.365e-04 | PDE=7.983e-04 | BC=2.649e-06 | IC=8.922e-06 | IF=1.606e-05\nEpoch  3500 | Total Loss=7.651e-04 | PDE=6.153e-04 | BC=1.901e-06 | IC=8.797e-06 | IF=1.928e-05\nEpoch  4000 | Total Loss=1.390e-02 | PDE=5.294e-04 | BC=1.900e-04 | IC=1.575e-04 | IF=2.327e-03\nEpoch  4500 | Total Loss=2.725e-03 | PDE=8.273e-04 | BC=1.378e-05 | IC=2.463e-05 | IF=3.411e-04\nEpoch  5000 | Total Loss=2.661e-02 | PDE=3.592e-04 | BC=2.850e-04 | IC=2.014e-04 | IF=4.764e-03\nEpoch  5500 | Total Loss=1.172e-02 | PDE=3.800e-04 | BC=5.155e-04 | IC=5.312e-04 | IF=1.222e-03\nEpoch  6000 | Total Loss=2.708e-03 | PDE=6.095e-04 | BC=2.405e-05 | IC=5.038e-05 | IF=3.452e-04\nEpoch  6500 | Total Loss=8.317e-04 | PDE=3.715e-04 | BC=6.359e-06 | IC=8.814e-06 | IF=7.687e-05\nEpoch  7000 | Total Loss=6.314e-03 | PDE=4.425e-04 | BC=8.803e-05 | IC=6.739e-05 | IF=1.019e-03\nEpoch  7500 | Total Loss=7.205e-03 | PDE=6.289e-04 | BC=8.232e-05 | IC=6.855e-05 | IF=1.164e-03\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_47/1931255925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0ml_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_pde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_bc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_ic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_if\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0ml_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mopt_adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Nx,Nt=201,201\n",
        "\n",
        "xg = torch.linspace(0,1,Nx,device=device)\n",
        "tg = torch.linspace(0,1,Nt,device=device)\n",
        "X,T = torch.meshgrid(xg,tg,indexing=\"ij\")\n",
        "with torch.no_grad():\n",
        "    inp = torch.stack((X.flatten(),T.flatten()),1)\n",
        "    mask = (X.flatten() <= x_if)\n",
        "    u_pred = torch.zeros_like(X.flatten()).unsqueeze(1)\n",
        "    u_pred[mask]  = netL(inp[mask])\n",
        "    u_pred[~mask] = netR(inp[~mask])\n",
        "    U_pred = u_pred.view(Nx,Nt).cpu().numpy()\n",
        "\n",
        "U_exact = u_exact(X.cpu().numpy(),T.cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "fig=plt.figure(figsize=(12,5))\n",
        "ax1=fig.add_subplot(1,2,1)\n",
        "cf=ax1.contourf(T.cpu(),X.cpu(),U_exact,50)\n",
        "fig.colorbar(cf,ax=ax1)\n",
        "ax1.set_title(\"Exact\")\n",
        "ax1.set_xlabel(\"t\")\n",
        "ax1.set_ylabel(\"x\")\n",
        "ax2=fig.add_subplot(1,2,2)\n",
        "cf2=ax2.contourf(T.cpu(),X.cpu(),U_pred,50)\n",
        "fig.colorbar(cf2,ax=ax2)\n",
        "ax2.set_title(\"PINN split-x\")\n",
        "ax2.set_xlabel(\"t\")\n",
        "ax2.set_ylabel(\"x\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_cut(t_val, label):\n",
        "    x = xg.unsqueeze(1)\n",
        "    t = torch.full_like(x, t_val)\n",
        "    with torch.no_grad():\n",
        "        up = torch.where(x <= x_if,\n",
        "                         netL(torch.cat((x, t), 1)),\n",
        "                         netR(torch.cat((x, t), 1)))\n",
        "\n",
        "    # PINN: red + markers\n",
        "    plt.plot(x.cpu(), up.cpu(), 'r+', markersize=4, label=f\"PINN {label}\")\n",
        "\n",
        "    # Exact: blue solid line\n",
        "    plt.plot(x.cpu(), u_exact(x.cpu().numpy(), np.full_like(x.cpu().numpy(), t_val)),\n",
        "             'b-', linewidth=1.5, label=f\"Exact {label}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_cut(0.0, 't=0')\n",
        "plot_cut(0.5, 't=0.5')\n",
        "plot_cut(1.0, 't=1')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"u\")\n",
        "plt.title(\"PINN vs Exact at Selected Time Slices\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-11T23:22:49.727734Z",
          "iopub.status.idle": "2025-12-11T23:22:49.728061Z",
          "shell.execute_reply.started": "2025-12-11T23:22:49.727919Z",
          "shell.execute_reply": "2025-12-11T23:22:49.727933Z"
        },
        "id": "U27M20q75Vws"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "qh992t0w5Vws"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "yloPjmXB5Vws"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "2-21emdh5Vws"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "FMho2lEZ5Vws"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}