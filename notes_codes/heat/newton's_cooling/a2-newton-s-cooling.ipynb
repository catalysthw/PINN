{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade \"protobuf<5.0.0\"\n\nimport google.protobuf\nprint(\"protobuf version:\", google.protobuf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### Newton's law of cooling (PINN) \n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n# Initial and boundary conditions \nT_amb = 27.0 # Ambient Temperature (¬∞C)\nT_o = 250.0  # Initial temperature at t = 0 (¬∞C)\nk = 0.45     # Cooling rate constant in the ODE dT/dt = k(T -T_amb)\n\n\n# NN configuration \n# Define the neural network model (w/ model parameters)\ndef create_model():\n    model = {\n        'hidden_1': tf.keras.layers.Dense(50, activation='tanh'),\n        'hidden_2': tf.keras.layers.Dense(50, activation='tanh'),\n        'hidden_3': tf.keras.layers.Dense(50, activation='tanh'),\n        'output_layer': tf.keras.layers.Dense(1, dtype='float32')\n    }\n    return model\n\n# function to call NN model \ndef call_model(model, t):\n    # Forward pass through the network: t -> hidden_1 -> hidden_... -> output_layer.\n    t = model['hidden_1'](t)\n    t = model['hidden_2'](t)\n    t = model['hidden_3'](t)\n    t = model['output_layer'](t)\n    \n    return t\n\n    \n# PDE residual: the differential equation\ndef pde(t, model):\n    \"\"\"\n    Compute the residual of the ODE:    dT/dt - k (T_amb - T) = 0\n\n    for the network prediction T_pred(t), the residual is:\n        f(t) = dT_pred/dt - k (T_amb - T_pred).\n    \"\"\"\n    \n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(t)\n        T_pred = call_model(model, t)    # Network approximation TÃÇ(t)\n        T_t = tape.gradient(T_pred, t)   # dT_hat/dt\n    \n    del tape\n    \n    return T_t - k * (T_amb -T_pred)\n\n\n\n# Loss function \ndef loss(model, t, t_ic, T_ic):\n    \"\"\"\n    Total loss = PDE residual loss (interior points)\n               + initial condition loss (t = 0).\n\n    t     : interior time points\n    t_ic  : time at the initial condition (here, [[0.0]])\n    T_ic  : initial temperature T(0) = T_o\n    \"\"\"\n    \n    # Residual of the OED at interior points\n    f = pde(t, model)\n    \n    # Mean squared error of the PDE residual\n    loss_pde = tf.reduce_mean(tf.square(f))\n    \n    # Netwrok prediction at the initial time\n    T_ic_pred = call_model(model, t_ic)\n    \n    # Mean squared error on the initial condition, T(0) = T_0\n    loss_ic = tf.reduce_mean(tf.square(T_ic - T_ic_pred))\n    \n    # Total loss \n    return loss_pde + loss_ic  # Totoal loss\n\n    # # Total loss with different weights on each loss term\n    # return 4*loss_pde + 2*loss_ic  \n\n\n\n# Training step\n\n# tf.function compiles this Python function into a TensorFlow graph,\n# which usually runs faster than eager execution for repeated calls.\n@tf.function\n\ndef train_step(model, t, t_ic, T_ic, optimizer):\n    \"\"\"\n    Perform a single optimization step:\n      1) compute total loss,\n      2) compute gradients w.r.t. all trainable variables,\n      3) update parameters using the optimizer.\n    \"\"\"\n    \n    with tf.GradientTape() as tape:\n        loss_value = loss(model, t, t_ic, T_ic)\n    \n    # Gradients for each layer's trainable variables\n    grads = tape.gradient(\n        loss_value,\n        [layer.trainable_variables for layer in model.values()])\n\n    # Flatten the nested list: [[g_W1, g_b1], [g_W2, g_b2], ...] -> [g_W1, g_b1, g_W2, ...]\n    grads = [g\n            for grad_list in grads\n            for g in grad_list]\n\n    # Collect the corresponding variables in the same order\n    variables = [var\n                 for layer in model.values()\n                 for var in layer.trainable_variables]\n\n    # Apply gradient updates (e.g., Adam step)\n    optimizer.apply_gradients(zip(grads, variables))\n\n    return loss_value\n\n\n# Interior time points t ‚àà [0, 10] used as collocation points for the ODE\nt_train = np.linspace(0, 10, 50).reshape(-1, 1)\nt_train = tf.convert_to_tensor(t_train, dtype=tf.float32)\n\n# Initial condition: T(0) = T_o\nt_ic = np.array([[0.0]], dtype=np.float32)\nT_ic = np.array([[T_o]], dtype=np.float32)\nt_ic = tf.convert_to_tensor(t_ic, dtype=tf.float32)\nT_ic = tf.convert_to_tensor(T_ic, dtype=tf.float32)\n\n\n## Model and optimizer\n# Instantiate the PINN\nmodel = create_model()\n\n# Adam optimizer \n\"\"\"\nLook inside '1d_Poisson eq.ipynb'. There are different optimizers in the code.\n(including two-stage optimization process)\n\"\"\"\n# Exponentially decaying learning rate for stable training\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-3,\n    decay_steps=500,\n    decay_rate=0.95\n)\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n\n## Main training loop\nepochs = 5000\nfor epoch in range(epochs):\n    loss_value = train_step(model, t_train, t_ic, T_ic, optimizer)\n    if epoch % 200 == 0:\n        print(f\"Epoch {epoch}: Loss = {loss_value.numpy()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:25:52.253872Z","iopub.execute_input":"2025-11-26T22:25:52.254553Z","iopub.status.idle":"2025-11-26T22:26:24.813802Z","shell.execute_reply.started":"2025-11-26T22:25:52.254517Z","shell.execute_reply":"2025-11-26T22:26:24.812934Z"}},"outputs":[{"name":"stderr","text":"2025-11-26 22:25:54.536869: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764195954.852323      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764195954.942527      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"2025-11-26 22:26:15.578313: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\nEpoch 0: Loss = 62646.11328125\nEpoch 200: Loss = 52397.4921875\nEpoch 400: Loss = 47728.5859375\nEpoch 600: Loss = 43681.58984375\nEpoch 800: Loss = 40068.62109375\nEpoch 1000: Loss = 36814.81640625\nEpoch 1200: Loss = 33873.1875\nEpoch 1400: Loss = 31208.578125\nEpoch 1600: Loss = 28792.447265625\nEpoch 1800: Loss = 26600.705078125\nEpoch 2000: Loss = 24612.48828125\nEpoch 2200: Loss = 22809.533203125\nEpoch 2400: Loss = 21175.609375\nEpoch 2600: Loss = 19696.23828125\nEpoch 2800: Loss = 18358.408203125\nEpoch 3000: Loss = 17150.37109375\nEpoch 3200: Loss = 16061.4384765625\nEpoch 3400: Loss = 15081.90625\nEpoch 3600: Loss = 14202.8466796875\nEpoch 3800: Loss = 13416.0888671875\nEpoch 4000: Loss = 12714.0849609375\nEpoch 4200: Loss = 12089.8388671875\nEpoch 4400: Loss = 11536.87109375\nEpoch 4600: Loss = 11049.154296875\nEpoch 4800: Loss = 10621.0263671875\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"## (Optional 1) Warm start: pre-training on initial conditions\n\n# warm_start_epochs = 500\n# for epoch in range(warm_start_epochs):\n#     loss_value = train_step(model, t_ic, t_ic, T_ic, optimizer)\n#     if epoch % 100 == 0:\n#         print(f\"Warm Start Epoch {epoch}: Loss = {loss_value.numpy()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## (Optional 2 approach) loss weight change\n\n# def loss_curriculum(model, t, t_ic, T_ic, w_pde=1.0, w_ic=1.0):\n#     \"\"\"\n#     Weighted loss:\n#         L = w_pde * L_pde + w_ic * L_ic\n#     \"\"\"\n#     # PDE residual\n#     f = pde(t, model)\n#     loss_pde = tf.reduce_mean(tf.square(f))\n\n#     # Initial condition\n#     T_ic_pred = call_model(model, t_ic)\n#     loss_ic = tf.reduce_mean(tf.square(T_ic - T_ic_pred))\n\n#     # early epochs: Set ùúÜ_IC large, ùúÜ_PDE smaller\n#     # later epochs: Gradually increase ùúÜ_PDE and/or decrease ùúÜ_IC \n#     return w_pde * loss_pde + w_ic * loss_ic\n\n\n# @tf.function\n# def train_step_curriculum(model, t, t_ic, T_ic, optimizer, w_pde, w_ic):\n#     \"\"\"\n#     One training step with curriculum weights for PDE and IC.\n#     \"\"\"\n#     with tf.GradientTape() as tape:\n#         loss_value = loss_curriculum(model, t, t_ic, T_ic, w_pde, w_ic)\n\n#     grads = tape.gradient(\n#         loss_value,\n#         [layer.trainable_variables for layer in model.values()]\n#     )\n#     grads = [g for grad_list in grads for g in grad_list]\n\n#     variables = [\n#         var\n#         for layer in model.values()\n#         for var in layer.trainable_variables\n#     ]\n\n#     optimizer.apply_gradients(zip(grads, variables))\n#     return loss_value\n\n\n# epochs = 5000\n# for epoch in range(epochs):\n#     \"\"\"\n#     Example) Simple curriculum:\n#             (1) first half: emphasize IC\n#             (2) second half: emphasize PDE\n#     \"\"\"\n#     if epoch < epochs // 2:\n#         w_ic = 5.0   # strong initial condition\n#         w_pde = 1.0\n#     else:\n#         w_ic = 1.0\n#         w_pde = 5.0  # stronger PDE residual\n\n#     loss_value = train_step_curriculum(\n#         model, t_train, t_ic, T_ic, optimizer, w_pde, w_ic\n#     )\n\n#     if epoch % 500 == 0:\n#         print(f\"[Curriculum] Epoch {epoch} \"\n#               f\"(w_pde={w_pde}, w_ic={w_ic}): loss = {loss_value.numpy():.6e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## (Optional 3 approach) Output transform to enforce IC/BC exactly (\"Hard constraints\")\n\n# # Define 'Raw' network + transform\n# def call_raw(model, t):\n#     \"\"\"\n#     Base network N(t). This does NOT yet enforce the initial condition.\n#     \"\"\"\n#     z = model['hidden_1'](t)\n#     z = model['hidden_2'](z)\n#     z = model['hidden_3'](z)\n#     out = model['output_layer'](z)\n#     return out\n\n# def call_model_constrained(model, t):\n#     \"\"\"\n#     Enforce T(0) = T_o by construction:\n\n#         T_hat(t) = T_o + t * N(t)\n\n#     No matter what N(t) is, T_hat(0) = T_o.\n#     \"\"\"\n#     N_t = call_raw(model, t)\n#     T_hat = T_o + t * N_t\n#     return T_hat\n\n\n# ## using T_hat for PDE residual\n# def pde_constrained(t, model):\n#     \"\"\"\n#     PDE residual using the constrained output T_hat(t):\n#         dT_hat/dt - k (T_amb - T_hat) = 0\n#     \"\"\"\n#     with tf.GradientTape(persistent=True) as tape:\n#         tape.watch(t)\n#         T_pred = call_model_constrained(model, t)\n#         T_t = tape.gradient(T_pred, t)\n\n#     del tape\n#     return T_t - k * (T_amb - T_pred)\n\n# # Without loss_IC because IC is already enforced.\n# def loss_constrained(model, t):\n#     \"\"\"\n#     Only PDE residual loss.\n#     The initial condition T(0) = T_o is automatically satisfied\n#     by the output transform.\n#     \"\"\"\n#     f = pde_constrained(t, model)\n#     loss_pde = tf.reduce_mean(tf.square(f))\n#     return loss_pde\n\n\n# @tf.function\n# def train_step_constrained(model, t, optimizer):\n#     with tf.GradientTape() as tape:\n#         loss_value = loss_constrained(model, t)\n\n#     grads = tape.gradient(\n#         loss_value,\n#         [layer.trainable_variables for layer in model.values()]\n#     )\n#     grads = [g for grad_list in grads for g in grad_list]\n\n#     variables = [\n#         var\n#         for layer in model.values()\n#         for var in layer.trainable_variables\n#     ]\n#     optimizer.apply_gradients(zip(grads, variables))\n#     return loss_value\n\n\n\n# epochs = 5000\n# for epoch in range(epochs):\n#     loss_value = train_step_constrained(model, t_train, optimizer)\n#     if epoch % 500 == 0:\n#         print(f\"[Hard IC] Epoch {epoch}: loss = {loss_value.numpy():.6e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## (Optional 4 approach) Pre-training with analytic solutions + PINN fine-tuning\n\n# def data_loss(model, t_data, T_data):\n#     \"\"\"\n#     Pure data-driven loss: MSE between PINN prediction and given data.\n#     \"\"\"\n#     T_pred = call_model(model, t_data)\n#     return tf.reduce_mean(tf.square(T_data - T_pred))\n\n\n# @tf.function\n# def train_step_data(model, t_data, T_data, optimizer):\n#     with tf.GradientTape() as tape:\n#         loss_value = data_loss(model, t_data, T_data)\n\n#     grads = tape.gradient(\n#         loss_value,\n#         [layer.trainable_variables for layer in model.values()]\n#     )\n#     grads = [g for grad_list in grads for g in grad_list]\n\n#     variables = [\n#         var\n#         for layer in model.values()\n#         for var in layer.trainable_variables\n#     ]\n#     optimizer.apply_gradients(zip(grads, variables))\n#     return loss_value\n\n# ## Stage 1: pre-training by using analytic solution\n# # Generate synthetic \"measurement\" data from the analytical solution\n# t_data = np.linspace(0, 10, 100).reshape(-1, 1).astype(np.float32)\n# T_data = T_amb + (T_o - T_amb) * np.exp(-k * t_data)\n\n# t_data = tf.convert_to_tensor(t_data, dtype=tf.float32)\n# T_data = tf.convert_to_tensor(T_data, dtype=tf.float32)\n\n# optimizer_data = tf.keras.optimizers.Adam(learning_rate=1e-3)\n\n# pretrain_epochs = 2000\n# for epoch in range(pretrain_epochs):\n#     loss_value = train_step_data(model, t_data, T_data, optimizer_data)\n#     if epoch % 200 == 0:\n#         print(f\"[Stage 1: Data pretrain] Epoch {epoch}: loss = {loss_value.numpy():.6e}\")\n\n\n# ## Stage 2: Fine-tuning by using PINN loss \n# optimizer_pinn = tf.keras.optimizers.Adam(learning_rate=5e-4)\n\n# fine_tune_epochs = 5000\n# for epoch in range(fine_tune_epochs):\n#     loss_value = train_step(\n#         model,        # Í∏∞Ï°¥ train_step(PINN) ÏÇ¨Ïö©\n#         t_train,\n#         t_ic,\n#         T_ic,\n#         optimizer_pinn\n#     )\n#     if epoch % 500 == 0:\n#         print(f\"[Stage 2: PINN fine-tune] Epoch {epoch}: loss = {loss_value.numpy():.6e}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## PINN prediction \nt_test = np.linspace(0, 10, 1000).reshape(-1, 1)\nt_test = tf.convert_to_tensor(t_test, dtype=tf.float32)\nT_pred = call_model(model, t_test).numpy()\n\n## Analytical solution for comparison\n# Exact solution of dT/dt = -k (T - T_amb) with T(0) = T_o:\n#   T(t) = T_amb + (T_o - T_amb) * exp(-k t)\nT_true = T_amb + (T_o - T_amb) * np.exp(-k * t_test)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(t_test, T_true, 'b-', label='Analytical Solution')\nplt.plot(t_test, T_pred, 'r--', label='PINN Solution')\nplt.xlabel('Time (sec)')\nplt.ylabel('T (¬∞C)')\nplt.legend()\n# plt.title('Comparison of Analytical Solution and PINN Solution')\nplt.show()\n\n# Plot PDE residuals\n# pde_residuals = pde(t_test, model).numpy()\n# pde_error = abs(T_true - T_pred)*100/abs(T_true)\n# plt.figure()\n# plt.plot(t_test, pde_error)\n# plt.xlabel(\"Time (sec)\")\n# plt.ylabel(\"PDE Error (%)\")\n# # plt.title(\"PDE Residuals\")\n# plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}